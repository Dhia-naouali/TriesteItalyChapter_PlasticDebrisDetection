{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11436527,"sourceType":"datasetVersion","datasetId":7163561},{"sourceId":11475439,"sourceType":"datasetVersion","datasetId":7191993}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! git clone https://github.com/acolite/acolite.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:00.389192Z","iopub.execute_input":"2025-04-26T20:12:00.389594Z","iopub.status.idle":"2025-04-26T20:12:00.524492Z","shell.execute_reply.started":"2025-04-26T20:12:00.389558Z","shell.execute_reply":"2025-04-26T20:12:00.523401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n! pip install rasterio\n! pip install pyresample\n! pip install netCDF4\n!pip install dvc dagshub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:01.515582Z","iopub.execute_input":"2025-04-26T20:12:01.515927Z","iopub.status.idle":"2025-04-26T20:12:17.314514Z","shell.execute_reply.started":"2025-04-26T20:12:01.515899Z","shell.execute_reply":"2025-04-26T20:12:17.313499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport rasterio\nfrom rasterio.windows import Window\nfrom rasterio.windows import from_bounds\nfrom rasterio.enums import Resampling\nfrom rasterio.transform import Affine\nimport shutil\nimport numpy as np\nimport tempfile\nimport shutil\nfrom pyproj import Transformer\nimport gdown\nimport dask.dataframe as dd\nimport glob\nfrom tqdm import tqdm\nimport cv2\nimport dagshub\nfrom dagshub.upload import Repo\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:17.316108Z","iopub.execute_input":"2025-04-26T20:12:17.317110Z","iopub.status.idle":"2025-04-26T20:12:17.322736Z","shell.execute_reply.started":"2025-04-26T20:12:17.317082Z","shell.execute_reply":"2025-04-26T20:12:17.321913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from dagshub.auth import add_app_token\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ndagshub_username = user_secrets.get_secret(\"DAGSHUB_USERNAME\")\ndagshub_token = user_secrets.get_secret(\"DAGSHUB_TOKEN\")\nadd_app_token(dagshub_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:17.323826Z","iopub.execute_input":"2025-04-26T20:12:17.324108Z","iopub.status.idle":"2025-04-26T20:12:17.543626Z","shell.execute_reply.started":"2025-04-26T20:12:17.324081Z","shell.execute_reply":"2025-04-26T20:12:17.542706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ACOLITE_PATH = \"./acolite\"\nsys.path.append(ACOLITE_PATH)\n# Import acolite_run\nfrom acolite.acolite.acolite_run import acolite_run","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:36:35.772045Z","iopub.execute_input":"2025-04-26T19:36:35.772324Z","iopub.status.idle":"2025-04-26T19:36:36.935317Z","shell.execute_reply.started":"2025-04-26T19:36:35.772300Z","shell.execute_reply":"2025-04-26T19:36:36.934497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tile_annotations_id = '1JyrCJX_CqZYUu3iC643L3YP8wgJM65Sc'\noutput = \"LWR_S2A_MSIL1C_20181017T094011_N0500_R036_T33SXC_20230729T163213.parquet\"  # Name for the downloaded file\nurl = f\"https://drive.google.com/uc?id={tile_annotations_id}\"\ngdown.download(url, output, quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:25.600787Z","iopub.execute_input":"2025-04-26T20:12:25.601112Z","iopub.status.idle":"2025-04-26T20:12:28.982284Z","shell.execute_reply.started":"2025-04-26T20:12:25.601087Z","shell.execute_reply":"2025-04-26T20:12:28.981335Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_utm_limit(utm_x, utm_y,utm_crs, box_meters, pixel_size=10):\n    \"\"\"Compute WGS84 limit from UTM center for box_meters x box_meters.\"\"\"\n    #utm_crs = \"EPSG:32631\"  # UTM 31N\n    wgs84_crs = \"EPSG:4326\"\n    utm_to_wgs = Transformer.from_crs(utm_crs, wgs84_crs, always_xy=True)\n    \n    half_box = box_meters / 2\n    box_left = utm_x - half_box\n    box_right = utm_x + half_box\n    box_bottom = utm_y - half_box\n    box_top = utm_y + half_box\n    \n    lon_min, lat_min = utm_to_wgs.transform(box_left, box_bottom)\n    lon_max, lat_max = utm_to_wgs.transform(box_right, box_top)\n    limit = [lat_min, lon_min, lat_max, lon_max]\n    \n    print(f\"UTM box: left={box_left:.1f}, bottom={box_bottom:.1f}, right={box_right:.1f}, top={box_top:.1f}\")\n    print(f\"WGS84 limit: {limit}\")\n    return limit, (box_left, box_bottom, box_right, box_top)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:28.983707Z","iopub.execute_input":"2025-04-26T20:12:28.984187Z","iopub.status.idle":"2025-04-26T20:12:28.990586Z","shell.execute_reply.started":"2025-04-26T20:12:28.984163Z","shell.execute_reply":"2025-04-26T20:12:28.989665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# def crop_to_exact_size(input_file, output_file, target_width=256, target_height=256, annot_utm_x=None, annot_utm_y=None):\n#     with rasterio.open(input_file) as src:\n#         width, height = src.width, src.height\n#         print(f\"Input shape: {width}x{height}\")\n#         res = src.res[0]\n#         if annot_utm_x is not None and annot_utm_y is not None:\n#             x_offset = int((annot_utm_x - src.bounds.left) / res - target_width / 2)\n#             y_offset = int((src.bounds.top - annot_utm_y) / res - target_height / 2)\n#         else:\n#             x_offset = (width - target_width) // 2\n#             y_offset = (height - target_height) // 2\n#         x_offset = max(0, min(x_offset, width - target_width))\n#         y_offset = max(0, min(y_offset, height - target_height))\n#         window = Window(x_offset, y_offset, target_width, target_height)\n#         data = src.read(window=window)\n#         transform = src.window_transform(window)\n#         profile = src.profile\n#         profile.update(width=target_width, height=target_height, transform=transform)\n#         with rasterio.open(output_file, 'w', **profile) as dst:\n#             dst.write(data)\n#         print(f\"Cropped to: {target_width}x{target_height}\")\n#         print(f\"Output UTM bounds: left={transform[2]:.1f}, bottom={transform[5]-target_height*res:.1f}, \"\n#               f\"right={transform[2]+target_width*res:.1f}, top={transform[5]:.1f}\")\n#         return transform[2], transform[5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:29.219544Z","iopub.execute_input":"2025-04-26T20:12:29.219844Z","iopub.status.idle":"2025-04-26T20:12:29.225197Z","shell.execute_reply.started":"2025-04-26T20:12:29.219824Z","shell.execute_reply":"2025-04-26T20:12:29.223904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! ls /kaggle/input/litter-windrows-batch-cala\nsafe_files_dir ='/kaggle/input/litter-windrows-batch-cala'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:29.448584Z","iopub.execute_input":"2025-04-26T20:12:29.449410Z","iopub.status.idle":"2025-04-26T20:12:29.596698Z","shell.execute_reply.started":"2025-04-26T20:12:29.449375Z","shell.execute_reply":"2025-04-26T20:12:29.595498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_dataset_tiles(safe_files_dir):\n    tiles = os.listdir(safe_files_dir)\n    tiles = [str(t)+'.SAFE' for t in tiles]\n    return tiles","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:30.388175Z","iopub.execute_input":"2025-04-26T20:12:30.388518Z","iopub.status.idle":"2025-04-26T20:12:30.393929Z","shell.execute_reply.started":"2025-04-26T20:12:30.388488Z","shell.execute_reply":"2025-04-26T20:12:30.393041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List the tiles in the dataset\n#safe_files_dir = '/kaggle/input/litter-windrows-batch-cala'\n# tiles = os.listdir(safe_files_dir)\n# tiles = [str(t)+'.SAFE' for t in tiles]\ntiles = get_dataset_tiles(safe_files_dir)\ntiles","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:30.668269Z","iopub.execute_input":"2025-04-26T20:12:30.668599Z","iopub.status.idle":"2025-04-26T20:12:30.675152Z","shell.execute_reply.started":"2025-04-26T20:12:30.668576Z","shell.execute_reply":"2025-04-26T20:12:30.674487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def get_utm_zone(longitude):\n#     return int((longitude + 180) / 6) + 1\n\n# longitude = 15.65  # Example for Calabria\n# utm_zone = get_utm_zone(longitude)\n# print(f\"UTM Zone: {utm_zone}N\")  # Outputs UTM Zone for Northern Hemisphere\n\n# from pyproj import Transformer\n\n# lat, lon = 38.5, 16.5  # Example for Calabria\n# utm_zone = get_utm_zone(lon)\n# epsg_code = f\"EPSG:{32600 + utm_zone}\"  # UTM Northern Hemisphere Code\n\n# transformer = Transformer.from_crs(\"EPSG:4326\", epsg_code, always_xy=True)\n# utm_x, utm_y = transformer.transform(lon, lat)\n\n# print(f\"UTM Coordinates: X={utm_x}, Y={utm_y}, Zone={utm_zone}N\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:34.041596Z","iopub.execute_input":"2025-04-26T20:12:34.041947Z","iopub.status.idle":"2025-04-26T20:12:34.046634Z","shell.execute_reply.started":"2025-04-26T20:12:34.041923Z","shell.execute_reply":"2025-04-26T20:12:34.045768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_crs_and_bounds(safe_file):\n    #granule_path = os.path.join(os.path.join(safe_files_dir, tiles[0]), tiles[0]+'.SAFE/GRANULE/')\n    granule_path = os.path.join(safe_file, 'GRANULE/')\n    #print(granule_path)\n    granule_subdirs = os.listdir(granule_path)  \n    img_data_path = os.path.join(granule_path, os.path.join(granule_subdirs[0]),'IMG_DATA/')\n    jpg2_files = os.listdir(img_data_path)\n    #print('jpg2_files')\n    #print(jpg2_files)\n    B02_files = [f  for f in jpg2_files if f.endswith('B02.jp2')]\n    #print('B02_files')\n    #print(B02_files)\n    band_path = os.path.join(img_data_path, B02_files[0])\n    print(band_path)\n    with rasterio.open(band_path) as src:\n        print(f\"CRS: {src.crs}\")  # Should be EPSG:32631\n        print(f\"Bounds: {src.bounds}\")  # Exact UTM coordinates\n        data = src.read(1)\n        print(f\"B02 Valid Pixels: {np.sum(data > 0)} / {data.size}\")\n    return src.crs, src.bounds, src.transform, src.res","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:34.328358Z","iopub.execute_input":"2025-04-26T20:12:34.328702Z","iopub.status.idle":"2025-04-26T20:12:34.335569Z","shell.execute_reply.started":"2025-04-26T20:12:34.328680Z","shell.execute_reply":"2025-04-26T20:12:34.334683Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_all_tiles(files):\n    \"\"\"\n    Given the list of the parquet annotation file, builds the list of all the \n    annotated tiles.\n    \"\"\"\n    df = None\n    # Process 10 files per batch\n    batch_size = 10\n    tiles = []\n    for i in range(0, len(files), batch_size):\n        batch_files = files[i:i + batch_size]\n        \n        # Read batch with Dask (lazy loading)\n        ddf = dd.read_parquet(batch_files, engine='pyarrow')\n        \n        # Compute to pandas (triggers parallel read)\n        batch_df = ddf.compute()  # Now contains data from 10 files\n        tiles.append(batch_df['s2_product'].unique())\n       \n        print(f\"Batch {i//batch_size + 1}: {len(batch_df)} rows (from {len(batch_files)} files)\")\n    \n    tiles = np.unique(np.hstack(tiles))\n    tiles = [t.decode('utf-8') for t in tiles]\n    return tiles","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:34.611491Z","iopub.execute_input":"2025-04-26T20:12:34.611805Z","iopub.status.idle":"2025-04-26T20:12:34.618482Z","shell.execute_reply.started":"2025-04-26T20:12:34.611782Z","shell.execute_reply":"2025-04-26T20:12:34.617294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_jp2_ref_file(safe_file):\n    granule_path = os.path.join(safe_file, 'GRANULE/')\n    granule_subdirs = os.listdir(granule_path)  \n    img_data_path = os.path.join(granule_path, os.path.join(granule_subdirs[0]),'IMG_DATA/')\n    jpg2_files = os.listdir(img_data_path)\n    B02_files = [f  for f in jpg2_files if f.endswith('B02.jp2')]\n    if len(B02_files) == 0:\n        raise ValueError(f'No B2 band jpg2 file retrieve in the safe file {safe_file}')\n    else :\n        return os.path.join(img_data_path, B02_files[0])\n        \ndef check_for_invalid_data(file_path, bbox):\n    with rasterio.open(file_path) as src:\n        # Read the window\n        window = src.window(*bbox)\n        data = src.read(1, window=window)\n    \n    # Sentinel-2 often uses 0 for invalid pixels\n    invalid_mask = (data == 0)\n    \n    # Or sometimes very high values (like 65535 for uint16)\n    if data.dtype == np.uint16:\n        invalid_mask = invalid_mask | (data == 65535)\n    \n    has_invalid = invalid_mask.any()\n    \n    if has_invalid:\n        invalid_count = invalid_mask.sum()\n        print(f\"Found {invalid_count} invalid pixels in the bounding box\")\n    else:\n        print(\"No invalid pixels found in the bounding box\")\n    return has_invalid\n\ndef generate_tile_regions(safe_file_path):\n    \"\"\"\n    Returns regions splitting the tile image\n    in the form of (xmin, ymin, xmax, ymax)\n    \"\"\"\n    w = 10980\n    h = 10980\n    stepx = 2700\n    stepy = 2700\n    sx = 3000\n    sy = 3000\n    regions = []\n    for i in range(4):\n        for j in range(4):\n            regions.append((i*stepx, j*stepy, min(w, i*stepx + sx), min(h, j*stepy + sy)))\n    ref_file = find_jp2_ref_file(safe_file_path)\n    filtered_regions = []\n   \n    for r in regions :\n        invalid = check_for_invalid_data(ref_file, r)\n        if not invalid:\n            filtered_regions.append(r)\n    regions = {i : r for i, r in enumerate(filtered_regions)}\n    return regions\n\ndef assign_patches(tile_regions, patches):\n    splitted_regions = {}\n    for p in patches: \n        for idr, r in tile_regions.items():\n            inside = p[0] >= r[0] and p[1] >= r[1] and p[2] <= r[2] and p[3] <= r[3]\n            if inside:\n                if idr  not in splitted_regions:\n                    splitted_regions[idr] = [p]\n                else :\n                    splitted_regions[idr].append(p)\n                break\n    return splitted_regions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:34.928490Z","iopub.execute_input":"2025-04-26T20:12:34.929259Z","iopub.status.idle":"2025-04-26T20:12:34.941739Z","shell.execute_reply.started":"2025-04-26T20:12:34.929235Z","shell.execute_reply":"2025-04-26T20:12:34.940809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef build_tile_df(target, files):\n    \"\"\"\n    Retrieves all annotations for a given target tile.\n    \"\"\"\n    df = None\n    # Process 10 files per batch\n    batch_size = 10\n    tg_id_1 = target.split('_')[2]\n    tg_id_2 = '_'.join(target.split('_')[4:6])\n    for i in range(0, len(files), batch_size):\n        batch_files = files[i:i + batch_size]\n        \n        # Read batch with Dask (lazy loading)\n        ddf = dd.read_parquet(batch_files, engine='pyarrow')\n        \n        # Compute to pandas (triggers parallel read)\n        batch_df = ddf.compute()  # Now contains data from 10 files\n        batch_df[\"s2_product\"] = batch_df[\"s2_product\"].str.decode('utf-8')\n        batch_df = batch_df[batch_df[\"s2_product\"].str.contains(tg_id_1) & batch_df[\"s2_product\"].str.contains(tg_id_2)]\n        if not batch_df.empty:\n            if df is None:\n                df = batch_df.copy()\n            else:\n                df = pd.concat([df, batch_df], ignore_index=True)\n        print(f\"Batch {i//batch_size + 1}: {len(batch_df)} rows (from {len(batch_files)} files)\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:35.168485Z","iopub.execute_input":"2025-04-26T20:12:35.168823Z","iopub.status.idle":"2025-04-26T20:12:35.175880Z","shell.execute_reply.started":"2025-04-26T20:12:35.168800Z","shell.execute_reply":"2025-04-26T20:12:35.174933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def read_tile_df(target, tile_parquet_dir):\n    fpathname = os.path.join(tile_parquet_dir, f'LWR_{target[:-5]}.parquet')\n    ddf = dd.read_parquet(fpathname, engine='pyarrow')\n    return ddf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:35.374257Z","iopub.execute_input":"2025-04-26T20:12:35.374585Z","iopub.status.idle":"2025-04-26T20:12:35.379661Z","shell.execute_reply.started":"2025-04-26T20:12:35.374562Z","shell.execute_reply":"2025-04-26T20:12:35.378740Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef build_tiles_parquet_annotations(tiles, files):\n    \"\"\"\n    Retrieves all annotations for a given target tile.\n    \"\"\"\n    for target in tiles:\n        df = None\n        # Process 10 files per batch\n        batch_size = 10\n        tg_id_1 = target.split('_')[2]\n        tg_id_2 = '_'.join(target.split('_')[4:6])\n        for i in range(0, len(files), batch_size):\n            batch_files = files[i:i + batch_size]\n            \n            # Read batch with Dask (lazy loading)\n            ddf = dd.read_parquet(batch_files, engine='pyarrow')\n            \n            # Compute to pandas (triggers parallel read)\n            batch_df = ddf.compute()  # Now contains data from 10 files\n            batch_df[\"s2_product\"] = batch_df[\"s2_product\"].str.decode('utf-8')\n            batch_df = batch_df[batch_df[\"s2_product\"].str.contains(tg_id_1) & batch_df[\"s2_product\"].str.contains(tg_id_2)  &\n    (~batch_df[\"pixel_x\"].isna()) &\n    (~batch_df[\"pixel_y\"].isna()) &\n    (batch_df[\"pixel_x\"] != -999) &\n    (batch_df[\"pixel_y\"] != -999) ]\n            if not batch_df.empty:\n                if df is None:\n                    df = batch_df.copy()\n                else:\n                    df = pd.concat([df, batch_df], ignore_index=True)\n            print(f\"Batch {i//batch_size + 1}: {len(batch_df)} rows (from {len(batch_files)} files)\")\n        # Save to Parquet\n        df.to_parquet(f'LWR_{target[:-5]}.parquet')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:40.345453Z","iopub.execute_input":"2025-04-26T20:12:40.346179Z","iopub.status.idle":"2025-04-26T20:12:40.353689Z","shell.execute_reply.started":"2025-04-26T20:12:40.346153Z","shell.execute_reply":"2025-04-26T20:12:40.352581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef get_relative_path(target_path, start_path):\n    \"\"\"\n    Returns the relative path from start_path to target_path\n    \"\"\"\n    return os.path.relpath(target_path, start_path)\n\n\ndef get_all_files_recursive(directory):\n    file_paths = []\n    for root, dirs, files in os.walk(directory):\n        #print(files)\n        for file in files:\n            full_path = os.path.abspath(os.path.join(root, file))\n            if os.path.isfile(full_path):  # Check to ensure it's a file\n                file_paths.append(full_path)\n    return file_paths\n\ndef upload_to_dagshub(root_folder, src_folder, dst_folder, batch_id, dagshub_token):\n    \"\"\"\n    dst_folder: Must have the format './dest_folder',\n    where dst_folder is the destination folder for the images\n    relative to the root of the repository.\n    \n    root_folder: The relative path of src_folder with respect to root_folder\n    will determine the position of the uploaded file relative to the root\n    of the repository.\n    \"\"\"\n\n    # Defining the Repo & directory\n    add_app_token(dagshub_token)\n    repo = Repo(\"elena-andreini\", \"TriesteItalyChapter_PlasticDebrisDetection\")\n    ds = repo.directory(dst_folder)\n    files = get_all_files_recursive(src_folder)\n    print(f'uploading files {files}')\n    rel_files_paths = [get_relative_path(f, src_folder) for f in files]\n    print(f'relative files {rel_files_paths}')\n\n    for i in rel_files_paths:\n      ds.add(file=os.path.join(src_folder, i), path=f'./{i}')\n    \n    # Commit the changes\n    ds.commit(f'Adding batch {batch_id}  to {dst_folder} folder', versioning='dvc')\n    print(f'Uploaded batch {batch_id}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:40.574389Z","iopub.execute_input":"2025-04-26T20:12:40.574749Z","iopub.status.idle":"2025-04-26T20:12:40.583114Z","shell.execute_reply.started":"2025-04-26T20:12:40.574725Z","shell.execute_reply":"2025-04-26T20:12:40.582199Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Define settings form map-mapper paper\nsettings = {\n    # Ensure TIFF export\n    'l2r_export_geotiff': True,  \n    # delete .nc once made to geotiff\n    'l1r_delete_netcdf' : True,\n    'l2w_delete_netcdf' : True,\n    'verbosity': 5,  # Detailed logging\n    's2_target_res': 10,  # 10m resolution\n    'resampling_method': 'nearest' , # Set to nearest neighbor\n    'atmospheric_correction_method': 'dark_spectrum',\n    'dsf_exclude_bands' : ['B9', 'B10'],\n    # resolves issue with none type see this thread - https://odnature.naturalsciences.be/remsem/acolite-forum/viewtopic.php?t=319\n    'geometry_type' : 'grids',\n    #masking\n    'l2w_mask' : False,\n    #sunglint\n    'glint_correction' : True,\n    'dsf_residual_glint_correction' : True,\n    'dsf_residual_glint_correction_method' : 'alternative', # index error occuring with default\n    'dsf_residual_glint_wave_range' : [1500,2400],\n    'glint_force_band' : None,\n    'glint_mask_rhos_wave' : 1600,\n    'glint_mask_rhos_threshold' : 0.11,\n    'reproject' : False\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:40.864254Z","iopub.execute_input":"2025-04-26T20:12:40.864582Z","iopub.status.idle":"2025-04-26T20:12:40.870456Z","shell.execute_reply.started":"2025-04-26T20:12:40.864560Z","shell.execute_reply":"2025-04-26T20:12:40.869485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_tile_mask(tile_df):\n    \"\"\"\n    Generate mask for the whole tile.\n    Based on dhia's code.\n    Not filtering filaments by box_dims size at the moment\n    \"\"\"\n    mask = np.zeros((10980, 10980))\n    pixels = 0\n    #pb = tqdm(tile_df[tile_df[\"box_dims\"] == 3].iterrows())\n    pb = tqdm(tile_df.iterrows())\n    for index, row in pb:\n        x, y = row[\"pixel_x\"], row[\"pixel_y\"]\n        if not np.isnan(x) and not np.isnan(y):\n            pixels += 1\n            x, y = int(x), int(y)\n            mask[x, y] = 1\n    return mask, pixels\n    \ndef find_subregions_efficient(binary_image, subregion_size, threshold):\n    \"\"\"\n    Finds regions with debris pixels in the whole tile annotation mask.\n    Using integral images for large binary images.\n    \"\"\"\n    h, w = subregion_size\n    img_h, img_w = binary_image.shape\n    print(f'input maks shape : {binary_image.shape}')\n    # Convert to binary and create integral image\n    binary = (binary_image == 1).astype(np.uint8)\n    integral = cv2.integral(binary)\n    \n    subregions = []\n    covered = np.zeros_like(binary, dtype=bool)\n    \n    for y in range(0, img_h - h + 1, h):\n        for x in range(0, img_w - w + 1, w):\n            #if covered[y:y+h, x:x+w].any():\n            #    continue\n                \n            # Calculate sum using integral image\n            total = integral[y+h, x+w] - integral[y, x+w] - integral[y+h, x] + integral[y, x]\n            white_fraction = total / (h * w)\n            #print(f'white_fraction {white_fraction}')\n            if total > threshold:\n                subregions.append((x, y,  x + w,  y + h, total))\n                covered[y:y+h, x:x+w] = True\n                \n    return subregions\n\ndef minimal_1024_cover(small_regions):\n    \"\"\"\n    This function could be used to find larger regions around patches\n    for ACOLITE application.\n    DSF algorithm could be not completely reliable applied to regions of 2560x2560 m unless\n    they are very homogeneous\n    \"\"\"\n    # Converti le coordinate in celle della griglia 256×256\n    cells = set((x // 1024, y // 1024) for (y, x, _, _,_) in small_regions)\n    if not cells:\n        return []\n    large_regions = set()\n    for (i, j) in cells:\n        large_i = i * 1024\n        large_j = j * 1024\n        large_regions.add((large_i, large_j))\n    return large_regions\n\n\ndef upload_to_drive(file_path, drive_folder_id):\n    \"\"\"Upload file to Google Drive.\"\"\"\n    output = gdown.upload(file_path, parent_id=drive_folder_id, quiet=False)\n    print(f\"Uploaded {file_path} to Drive folder {drive_folder_id}\")\n    os.remove(file_path)  # Clear Kaggle disk\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:41.296588Z","iopub.execute_input":"2025-04-26T20:12:41.297383Z","iopub.status.idle":"2025-04-26T20:12:41.308390Z","shell.execute_reply.started":"2025-04-26T20:12:41.297356Z","shell.execute_reply":"2025-04-26T20:12:41.307392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate_image(image):\n    # Define invalid values (NaN, Inf, or negative)\n    invalid_mask = np.isnan(image) | np.isinf(image) | (image < 0)\n    total_pixels = image.size\n    invalid_count = invalid_mask.sum()\n    invalid_percentage = (invalid_count / total_pixels) * 100\n    print(f\"Percentage of invalid values (NaN, Inf, negative) across all bands: {invalid_percentage:.2f}%\")\n    if invalid_percentage > 10:\n        print('skipping region, too many invalid pixels ')\n        return False\n    else :\n        return True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:48.409852Z","iopub.execute_input":"2025-04-26T20:12:48.410177Z","iopub.status.idle":"2025-04-26T20:12:48.415542Z","shell.execute_reply.started":"2025-04-26T20:12:48.410155Z","shell.execute_reply":"2025-04-26T20:12:48.414620Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#### Stacking \n\n\ndef stack_bands(acolite_output_dir, stacked_dir, tile):\n    # Mapping wawelength to band names\n    wl_to_band  ={\n        443 : 'B1',\n        492 : 'B2',\n        560 : 'B3',\n        665 : 'B4',\n        704 : 'B5',\n        740 : 'B6',\n        783 : 'B7',\n        833 : 'B8',\n        865 : 'B8A',\n        1614 : 'B11',\n        2202 : 'B12'\n    }\n    rhos_files = [f for f in os.listdir(acolite_output_dir) if 'rhos' in f and '.tif' in f]\n    rhos_files = sorted(rhos_files, key=lambda x: int(x.split('_')[-1][:-4]))\n    band_files = [(wl_to_band[int(f.split('_')[-1][:-4])], f) for i,f in enumerate(rhos_files) ]\n\n    # Verify all files exist\n    for band, filename in band_files:\n        if not os.path.exists(os.path.join(acolite_output_dir, filename)):\n            raise FileNotFoundError(f\"Missing file: {filename}\")\n\n    # Reference band (use B4 for metadata, since all bands are 10m)\n    reference_band = 'B4'\n    reference_file = os.path.join(acolite_output_dir, [f for b, f in band_files if b == reference_band][0])\n\n    # Open the reference GeoTIFF to get metadata\n    with rasterio.open(reference_file) as ref:\n        ref_profile = ref.profile\n        ref_transform = ref.transform\n        ref_crs = ref.crs\n        ref_width = ref.width\n        ref_height = ref.height\n        ref_resolution = ref.res  # Should be (10.0, 10.0)\n\n    # Initialize an array to store all bands\n    stacked_data = np.zeros((len(band_files), ref_height, ref_width), dtype=np.float32)\n    \n    # Read each band (no resampling needed, all are 10m)\n    for i, (band, filename) in enumerate(band_files):\n        file_path = os.path.join(acolite_output_dir, filename)\n        with rasterio.open(file_path) as src:\n            # Verify resolution matches\n            if src.res != ref_resolution:\n                raise ValueError(f\"Resolution mismatch in {filename}: expected {ref_resolution}, got {src.res}\")\n            # Verify dimensions match\n            if src.width != ref_width or src.height != ref_height:\n                raise ValueError(f\"Dimensions mismatch in {filename}: expected {ref_width}x{ref_height}, got {src.width}x{src.height}\")\n            # Read the band\n            data = src.read(1, out_dtype=np.float32)  # Single band, float32\n            stacked_data[i] = data\n\n    # Update the profile for the stacked GeoTIFF\n    stacked_profile = ref_profile.copy()\n    stacked_profile.update({\n        'count': len(band_files),  # 11 bands\n        'dtype': np.float32,  # For reflectance\n        'transform': ref_transform,\n        'crs': ref_crs,\n        'width': ref_width,\n        'height': ref_height,\n        'nodata': -999  # Optional: Set nodata value (adjust if ACOLITE uses NaN)\n    })\n    # Determine patch name similar to MARIDA : S2_DATE_TILE_REGION in folder S2_DATE_TILE\n    stacked_tif_fname =  'S2_'+('_'.join(tile.split('_')[2:-3]))+'_stacked.tiff'\n    stacked_tif = os.path.join(stacked_dir, stacked_tif_fname)\n    # Save the stacked GeoTIFF\n    with rasterio.open(stacked_tif, 'w', **stacked_profile) as dst:\n        dst.write(stacked_data)\n        # Set band descriptions in the usual order\n        for i, (band, _) in enumerate(band_files, 1):\n            dst.set_band_description(i, band)\n    \n    print(f\"Stacked GeoTIFF saved to: {stacked_tif}\")\n\n    # Verify the stacked GeoTIFF\n    with rasterio.open(stacked_tif) as stacked:\n        print(f\"Stacked GeoTIFF:\")\n        print(f\"Bounds: {stacked.bounds}\")\n        print(f\"Dimensions: {stacked.width} columns, {stacked.height} rows\")\n        print(f\"Resolution: {stacked.res}\")\n        print(f\"CRS: {stacked.crs}\")\n        print(f\"Number of bands: {stacked.count}\")\n        print(f\"Band descriptions: {stacked.descriptions}\")\n    valid = validate_image(stacked_data)\n    return stacked_tif, valid","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:48.657372Z","iopub.execute_input":"2025-04-26T20:12:48.658241Z","iopub.status.idle":"2025-04-26T20:12:48.671065Z","shell.execute_reply.started":"2025-04-26T20:12:48.658215Z","shell.execute_reply":"2025-04-26T20:12:48.670062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"######## Cropping\n\n\n# Paths to the original GeoTIFF, stacked GeoTIFF, and output\ndef crop_stacked_file(stacked_tif, cropped_tif, utm_box, crs, tile_res):\n    \n    # Computing metadata for the original patch (before ACOLITE correction)\n    \n    initial_bounds = utm_box  \n    original_width = 256\n    original_height = 256\n    original_transform = Affine.translation(utm_box[0], utm_box[3]) * Affine.scale(10.0, -10.0)\n    original_crs = crs\n    original_resolution = tile_res\n\n    # Open the stacked GeoTIFF and crop to initial bounds\n    with rasterio.open(stacked_tif) as src:\n        print(f'stacked file bounds {src.bounds}')\n        # Define a window based on the initial bounds\n        window = from_bounds(*initial_bounds, transform=src.transform)\n\n        # Sanity check\n        \n        # Verify the window is within the subregion's dimensions\n        assert 0 <= window.col_off < src.width, \"Window column offset out of bounds\"\n        assert 0 <= window.row_off < src.height, \"Window row offset out of bounds\"\n        assert window.width > 0 and window.col_off + window.width <= src.width, \"Window width out of bounds\"\n        assert window.height > 0 and window.row_off + window.height <= src.height, \"Window height out of bounds\"\n        \n        # Read the data, matching original dimensions\n        window_data = src.read(\n            window=window#,\n            #out_shape=(src.count, original_height, original_width),\n            #resampling=Resampling.nearest  # Nearest neighbor to avoid interpolation\n        )\n        \n        # Get band descriptions to preserve metadata\n        band_descriptions = src.descriptions\n        \n        # Update the transform to match the original GeoTIFF\n        window_transform = original_transform\n        \n        # Update the profile with new metadata\n        profile = src.profile\n        profile.update({\n            'width': original_width,\n            'height': original_height,\n            'transform': window_transform,\n            'crs': original_crs\n        })\n        \n        # Create a new GeoTIFF with the cropped data\n        with rasterio.open(cropped_tif, 'w', **profile) as dst:\n            dst.write(window_data)\n            # Preserve band descriptions (e.g., B1, B2, ...)\n            for i, desc in enumerate(band_descriptions, 1):\n                if desc:  # Only set if description exists\n                    dst.set_band_description(i, desc)\n    \n    print(f\"Cropped GeoTIFF saved to: {cropped_tif}\")\n\n    # Verify the cropped GeoTIFF\n    with rasterio.open(cropped_tif) as cropped:\n        print(f\"Cropped GeoTIFF:\")\n        print(f\"Bounds: {cropped.bounds}\")\n        print(f\"Dimensions: {cropped.width} columns, {cropped.height} rows\")\n        print(f\"Resolution: {cropped.res}\")\n        print(f\"CRS: {cropped.crs}\")\n        print(f\"Number of bands: {cropped.count}\")\n        print(f\"Band descriptions: {cropped.descriptions}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:48.874343Z","iopub.execute_input":"2025-04-26T20:12:48.875003Z","iopub.status.idle":"2025-04-26T20:12:48.884467Z","shell.execute_reply.started":"2025-04-26T20:12:48.874977Z","shell.execute_reply":"2025-04-26T20:12:48.883610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##### Run if you want to extract all tiles from the LW annotation file\n#%%time\n#all_tiles = get_all_tiles(annotations_files)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:12:52.393513Z","iopub.execute_input":"2025-04-26T20:12:52.393812Z","iopub.status.idle":"2025-04-26T20:12:52.398078Z","shell.execute_reply.started":"2025-04-26T20:12:52.393790Z","shell.execute_reply":"2025-04-26T20:12:52.397112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_tile(target, safe_files_dir, annotations_files, output_dir, remote_dataset_location = './LWC_DataSet/test_data', batch_id='', clean_patches = True, dagshub_token=''):\n    acolite_output_dir = os.path.join(output_dir, 'acolite_output/')\n    stacked_dir = os.path.join(output_dir, 'stacked/')\n    final_dir = os.path.join(output_dir, 'patches/')\n    os.makedirs(acolite_output_dir, exist_ok=True)\n    os.makedirs(stacked_dir, exist_ok=True)\n    if clean_patches : \n        shutil.rmtree(acolite_output_dir, ignore_errors=True)\n    os.makedirs(final_dir, exist_ok=True)\n    print(f'building annotations dataframe for tile {target}')\n    tile_df = build_tile_df(target, annotations_files)\n    #tile_df = read_tile_df(target, '/kaggle/working')\n    mask, debris_pixels_count = generate_tile_mask(tile_df)\n    print(f'tile mask shape {mask.shape}')\n    if debris_pixels_count == 0:\n        print(f'no valid debris pixels in tile {target}')\n        return False\n    patches = find_subregions_efficient(mask, (256, 256), 0)\n    print(f'patches with marine debris : {patches}')\n    target_file_path = os.path.join(os.path.join(safe_files_dir, target[:-5]), target)\n    crs, bounds, tile_transform, tile_res = extract_crs_and_bounds(target_file_path)       \n    print(f'crs : {crs}')\n    print(f'bounds : {bounds}')\n    res = tile_res[0] #sentinel 2 max resolution\n    tile_regions = generate_tile_regions(target_file_path)\n    patches_per_region = assign_patches(tile_regions, patches)\n    print(f'patches per region {patches_per_region}')\n    for i, ppr in patches_per_region.items(): #iterates over regions containing patches\n        shutil.rmtree(stacked_dir, ignore_errors=True)\n        os.makedirs(stacked_dir, exist_ok=True)\n        current_region = tile_regions[i]\n        current_center = ((current_region[0] + current_region[2])/2 * res + bounds.left,\n        (current_region[1] + current_region[3])/2 * (-res) + bounds.top)\n        #subregion_centers = [((int(p[1] + p[3])/2) * res + bounds.left, int((p[0] + p[2])/2) * res + bounds.bottom) for p in patches]\n        print(f'region center : {current_center}')\n        #subregion_size = (256 * res, 256 * res)\n        current_region_size = ((current_region[2] - current_region[0]) * res, \n                               (current_region[3] - current_region[1]) * res )\n        try:\n            # Check disk usage\n            print(\"Disk usage before run:\")\n            !du -sh /kaggle/input/*\n            !df -h /kaggle/tmp /kaggle/working\n        \n            # Clear output directory\n            shutil.rmtree(acolite_output_dir, ignore_errors=True)\n            os.makedirs(acolite_output_dir, exist_ok=True)\n\n\n        # Process each subregion\n        #for i, (subregion_utm_x, subregion_utm_y) in enumerate(subregion_centers):\n            # Clear output directory\n            subregion_utm_x, subregion_utm_y = current_center\n            print(f'Cleaning previous acolite output')\n            shutil.rmtree(acolite_output_dir, ignore_errors=True)\n            os.makedirs(acolite_output_dir, exist_ok=True)\n            print(f\"\\nProcessing subregion {i+1}/{len(tile_regions)} centered at UTM ({subregion_utm_x}, {subregion_utm_y})\")\n    \n            # Compute UTM-based limit\n            limit, utm_box = get_utm_limit(subregion_utm_x, subregion_utm_y, crs, current_region_size[0]) #subregion_size[0])\n            print(f'setting limit : {limit}')\n            settings['limit'] = limit\n            print(f\"Settings for subregion {i+1}: {settings}\")\n            output_files = acolite_run(settings=settings, inputfile=target_file_path, output=acolite_output_dir)\n            print(f\"Generated output files {output_files}\")\n            print('stacking bands')\n            stacked_tiff, valid = stack_bands(acolite_output_dir, stacked_dir, target)\n            if not valid : \n                print(f'skipping region {current_region} : too many invalid pixels')\n                continue\n            patch_dir =  os.path.join(final_dir, '_'.join((stacked_tiff.split('/')[-1]).split('_')[:-1]))# '_'.join(stacked_tiff.split('_')[:-1])\n            print(f'patch dir {patch_dir}')\n            for p in ppr:\n                print(f'cropping  patch {p}')\n                patch_utm_bound = (p[0] * res + bounds.left, (p[3] - 1) * (-res) + bounds.top,\n                                   p[2] * res + bounds.left,  p[1] * (-res) + bounds.top)\n                patch_name_prefix = patch_dir.split('/')[-1]\n                patch_name = f'{patch_name_prefix}_{int(patch_utm_bound[0])}_{int(patch_utm_bound[1])}.tiff'\n                patch_pathname = os.path.join(patch_dir, patch_name)\n                os.makedirs(patch_dir, exist_ok=True)\n                print(f'patch pathname {patch_pathname}')\n                crop_stacked_file(stacked_tiff, os.path.join(patch_dir, patch_pathname), patch_utm_bound, crs, tile_res)\n                patch_mask = mask[p[1] : p[3], p[0] : p[2]]\n                print(f'patch mask shape {patch_mask.shape}')\n                try:\n                    cv2.imwrite(os.path.join(patch_dir, f'{patch_pathname[:-4]}_cl.tif'), patch_mask)\n                except Exception as e:\n                    print(f\"Error writing mask: {e}\")\n            # Insert Sanity Check for ACOLITE output ? \n            \n            # Upload to DagsHub\n            if dagshub_token:\n                upload_to_dagshub(output_dir, final_dir, remote_dataset_location, batch_id, dagshub_token)\n    \n    \n\n            # Final disk usage\n            print(\"\\nDisk usage after run:\")\n            ! du -sh /kaggle/input/*\n            !df -h /kaggle/tmp /kaggle/working\n      \n    \n        except Exception as e:\n            print(f\"Exception captured: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:21:40.757942Z","iopub.execute_input":"2025-04-26T20:21:40.758623Z","iopub.status.idle":"2025-04-26T20:21:40.799063Z","shell.execute_reply.started":"2025-04-26T20:21:40.758596Z","shell.execute_reply":"2025-04-26T20:21:40.798089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Setting the directory of the annotations file split and converted to parquet format\nparquet_annotations_dir = '/kaggle/input/lw-parquet/kaggle/working'\nannotations_files = [os.path.join(parquet_annotations_dir, f) for f in os.listdir(parquet_annotations_dir)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:21:47.164619Z","iopub.execute_input":"2025-04-26T20:21:47.165360Z","iopub.status.idle":"2025-04-26T20:21:47.185206Z","shell.execute_reply.started":"2025-04-26T20:21:47.165329Z","shell.execute_reply":"2025-04-26T20:21:47.184316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T20:59:25.668520Z","iopub.execute_input":"2025-04-25T20:59:25.669480Z","iopub.status.idle":"2025-04-25T20:59:25.847006Z","shell.execute_reply.started":"2025-04-25T20:59:25.669448Z","shell.execute_reply":"2025-04-25T20:59:25.845813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#build_tiles_parquet_annotations([tiles[5]], annotations_files)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T20:59:32.999815Z","iopub.execute_input":"2025-04-25T20:59:33.000264Z","iopub.status.idle":"2025-04-25T20:59:33.005627Z","shell.execute_reply.started":"2025-04-25T20:59:33.000221Z","shell.execute_reply":"2025-04-25T20:59:33.004817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_batch(tiles, safe_files_dir, annotations_files, batch_id='', dagshub_token=''):\n    \"\"\"\n    tiles :  list of tiles (SAFE files) in batch batch_id\n    \"\"\"\n    clean_patches = len(dagshub_token) != 0\n    for target in tiles:\n        process_tile(target, safe_files_dir, annotations_files, '/kaggle/working', batch_id, clean_patches =clean_patches, dagshub_token=dagshub_token)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:21:50.098927Z","iopub.execute_input":"2025-04-26T20:21:50.099239Z","iopub.status.idle":"2025-04-26T20:21:50.105106Z","shell.execute_reply.started":"2025-04-26T20:21:50.099216Z","shell.execute_reply":"2025-04-26T20:21:50.104157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_batch([tiles[5]],safe_files_dir, annotations_files,  batch_id='', dagshub_token='')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T20:21:59.260963Z","iopub.execute_input":"2025-04-26T20:21:59.261279Z","iopub.status.idle":"2025-04-26T20:26:43.657211Z","shell.execute_reply.started":"2025-04-26T20:21:59.261256Z","shell.execute_reply":"2025-04-26T20:26:43.655831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! ls ./stacked","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T21:08:20.013241Z","iopub.execute_input":"2025-04-25T21:08:20.013808Z","iopub.status.idle":"2025-04-25T21:08:20.205411Z","shell.execute_reply.started":"2025-04-25T21:08:20.013744Z","shell.execute_reply":"2025-04-25T21:08:20.204104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = dd.read_parquet('/kaggle/working/LWR_S2A_MSIL1C_20181017T094011_N0500_R036_T33SXC_20230729T163213.parquet')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T21:07:23.992537Z","iopub.execute_input":"2025-04-25T21:07:23.995493Z","iopub.status.idle":"2025-04-25T21:07:24.027872Z","shell.execute_reply.started":"2025-04-25T21:07:23.995447Z","shell.execute_reply":"2025-04-25T21:07:24.026954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nupload_to_dagshub('/kaggle/working', '/kaggle/working/patches', './LWC_DataSet/test_data', \"\", dagshub_token=dagshub_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T16:04:52.761393Z","iopub.execute_input":"2025-04-23T16:04:52.762482Z","iopub.status.idle":"2025-04-23T16:05:58.635035Z","shell.execute_reply.started":"2025-04-23T16:04:52.762390Z","shell.execute_reply":"2025-04-23T16:05:58.634321Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! mv  ./patches/S2_20181017T094011_N0500/S2_20181017T094011_N0500_610240_4264460.tiff ./patches/S2_20181017T094011_N0500/S2_20181017T094011_N0500_610240_4264460.tif","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:43:35.892088Z","iopub.execute_input":"2025-04-23T15:43:35.892460Z","iopub.status.idle":"2025-04-23T15:43:36.075280Z","shell.execute_reply.started":"2025-04-23T15:43:35.892430Z","shell.execute_reply":"2025-04-23T15:43:36.074059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_batch([tiles[5]],safe_files_dir, annotations_files,  batch_id='', dagshub_token=dagshub_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:32:27.725905Z","iopub.execute_input":"2025-04-23T15:32:27.726255Z","iopub.status.idle":"2025-04-23T15:40:21.928934Z","shell.execute_reply.started":"2025-04-23T15:32:27.726230Z","shell.execute_reply":"2025-04-23T15:40:21.927801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_tile(tiles[7], safe_files_dir, annotations_files, '/kaggle/working', batch_id)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-23T15:25:49.447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! ls /kaggle/working/patches/S2_20181017T094011_N0500","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T22:36:17.229979Z","iopub.execute_input":"2025-04-21T22:36:17.230391Z","iopub.status.idle":"2025-04-21T22:36:17.468027Z","shell.execute_reply.started":"2025-04-21T22:36:17.230360Z","shell.execute_reply":"2025-04-21T22:36:17.466571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os.path\n\ndef get_relative_path(target_path, start_path):\n    \"\"\"\n    Returns the relative path from start_path to target_path\n    \"\"\"\n    return os.path.relpath(target_path, start_path)\n\n# Example usage:\nrelative_path = get_relative_path(\"/full/path/to/file\", \"/full/path\")\nprint(relative_path)  # Output: to/file","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:05:46.051893Z","iopub.execute_input":"2025-04-23T09:05:46.052219Z","iopub.status.idle":"2025-04-23T09:05:46.064936Z","shell.execute_reply.started":"2025-04-23T09:05:46.052191Z","shell.execute_reply":"2025-04-23T09:05:46.063956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:30:51.699733Z","iopub.execute_input":"2025-04-23T09:30:51.700094Z","iopub.status.idle":"2025-04-23T09:30:51.828849Z","shell.execute_reply.started":"2025-04-23T09:30:51.700048Z","shell.execute_reply":"2025-04-23T09:30:51.827471Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! mkdir ./patches\n! mkdir ./patches/S2_patch1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:22:53.899835Z","iopub.execute_input":"2025-04-23T12:22:53.900214Z","iopub.status.idle":"2025-04-23T12:22:54.164615Z","shell.execute_reply.started":"2025-04-23T12:22:53.900188Z","shell.execute_reply":"2025-04-23T12:22:54.163292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\n\nim = np.random.rand(256, 256)\nmask = np.random.rand(256, 256)\ncv2.imwrite('./patches/S2_patch1/image.png', im)\ncv2.imwrite('./patches/S2_patch1/mask.png', mask)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:22:55.027778Z","iopub.execute_input":"2025-04-23T12:22:55.028267Z","iopub.status.idle":"2025-04-23T12:22:55.055910Z","shell.execute_reply.started":"2025-04-23T12:22:55.028233Z","shell.execute_reply":"2025-04-23T12:22:55.055041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! dagshub login","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:30:08.744672Z","iopub.execute_input":"2025-04-23T09:30:08.745070Z","iopub.status.idle":"2025-04-23T09:30:26.322538Z","shell.execute_reply.started":"2025-04-23T09:30:08.745032Z","shell.execute_reply":"2025-04-23T09:30:26.321200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_all_files_recursive('/kaggle/working/patches')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:33:22.904914Z","iopub.execute_input":"2025-04-23T09:33:22.905852Z","iopub.status.idle":"2025-04-23T09:33:22.912206Z","shell.execute_reply.started":"2025-04-23T09:33:22.905815Z","shell.execute_reply":"2025-04-23T09:33:22.911404Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! ls ./kaggle/working/patches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:32:54.226896Z","iopub.execute_input":"2025-04-23T09:32:54.227315Z","iopub.status.idle":"2025-04-23T09:32:54.370147Z","shell.execute_reply.started":"2025-04-23T09:32:54.227288Z","shell.execute_reply":"2025-04-23T09:32:54.368933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"upload_to_dagshub('/kaggle/working', '/kaggle/working/patches', './test_data/patches', 'batch0')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:33:58.232386Z","iopub.execute_input":"2025-04-23T12:33:58.232726Z","iopub.status.idle":"2025-04-23T12:33:59.046225Z","shell.execute_reply.started":"2025-04-23T12:33:58.232705Z","shell.execute_reply":"2025-04-23T12:33:59.044768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from dagshub.upload import Repo\nimport os\n\n\n\ndef get_relative_path(target_path, start_path):\n    \"\"\"\n    Returns the relative path from start_path to target_path\n    \"\"\"\n    return os.path.relpath(target_path, start_path)\n\n\ndef get_all_files_recursive(directory):\n    file_paths = []\n    for root, dirs, files in os.walk(directory):\n        #print(files)\n        for file in files:\n            full_path = os.path.abspath(os.path.join(root, file))\n            if os.path.isfile(full_path):  # Check to ensure it's a file\n                file_paths.append(full_path)\n    return file_paths\n\ndef upload_to_dagshub(root_folder, src_folder, dst_folder, batch_id):\n    \"\"\"\n    dst_folder: Must have the format './dest_folder',\n    where dst_folder is the destination folder for the images\n    relative to the root of the repository.\n    \n    root_folder: The relative path of src_folder with respect to root_folder\n    will determine the position of the uploaded file relative to the root\n    of the repository.\n    \"\"\"\n\n    # Defining the Repo & directory\n    repo = Repo(\"elena-andreini\", \"TriesteItalyChapter_PlasticDebrisDetection\")\n    ds = repo.directory(dst_folder)\n    files = get_all_files_recursive(src_folder)\n    print(f'uploading files {files}')\n    rel_files_paths = [get_relative_path(f, src_folder) for f in files]\n    print(f'relative files {rel_files_paths}')\n\n    for i in rel_files_paths:\n      ds.add(file=os.path.join(src_folder, i), path=f'./{i}')\n    \n    # Commit the changes\n    ds.commit(f'Adding batch {batch_id}  to {dst_folder} folder', versioning='dvc')\n    print(f'Uploaded batch {batch_id}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:40:53.744680Z","iopub.execute_input":"2025-04-23T09:40:53.745052Z","iopub.status.idle":"2025-04-23T09:40:53.759512Z","shell.execute_reply.started":"2025-04-23T09:40:53.745030Z","shell.execute_reply":"2025-04-23T09:40:53.758138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}