{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-01T22:06:01.453446Z",
     "iopub.status.busy": "2025-05-01T22:06:01.452984Z",
     "iopub.status.idle": "2025-05-01T22:06:25.623216Z",
     "shell.execute_reply": "2025-05-01T22:06:25.620550Z",
     "shell.execute_reply.started": "2025-05-01T22:06:01.453412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'acolite'...\n",
      "remote: Enumerating objects: 13022, done.\u001b[K\n",
      "remote: Counting objects: 100% (2124/2124), done.\u001b[K\n",
      "remote: Compressing objects: 100% (339/339), done.\u001b[K\n",
      "remote: Total 13022 (delta 1962), reused 1850 (delta 1784), pack-reused 10898 (from 3)\u001b[K\n",
      "Receiving objects: 100% (13022/13022), 711.71 MiB | 35.05 MiB/s, done.\n",
      "Resolving deltas: 100% (8329/8329), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/acolite/acolite.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:06:25.625728Z",
     "iopub.status.busy": "2025-05-01T22:06:25.625214Z",
     "iopub.status.idle": "2025-05-01T22:07:07.995318Z",
     "shell.execute_reply": "2025-05-01T22:07:07.993791Z",
     "shell.execute_reply.started": "2025-05-01T22:06:25.625684Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install rasterio\n",
    "! pip install pyresample\n",
    "! pip install netCDF4\n",
    "! pip install dvc dagshub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:07:07.998169Z",
     "iopub.status.busy": "2025-05-01T22:07:07.997007Z",
     "iopub.status.idle": "2025-05-01T22:07:16.909050Z",
     "shell.execute_reply": "2025-05-01T22:07:16.908031Z",
     "shell.execute_reply.started": "2025-05-01T22:07:07.998124Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.transform import Affine\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tempfile\n",
    "from pyproj import Transformer\n",
    "import gdown\n",
    "import dask.dataframe as dd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import dagshub\n",
    "from dagshub.upload import Repo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:07:16.912829Z",
     "iopub.status.busy": "2025-05-01T22:07:16.911999Z",
     "iopub.status.idle": "2025-05-01T22:07:17.471718Z",
     "shell.execute_reply": "2025-05-01T22:07:17.468730Z",
     "shell.execute_reply.started": "2025-05-01T22:07:16.912797Z"
    }
   },
   "outputs": [],
   "source": [
    "from dagshub.auth import add_app_token\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "dagshub_username = user_secrets.get_secret(\"DAGSHUB_USERNAME\")\n",
    "dagshub_token = user_secrets.get_secret(\"DAGSHUB_TOKEN\")\n",
    "add_app_token(dagshub_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:07:17.473466Z",
     "iopub.status.busy": "2025-05-01T22:07:17.473122Z",
     "iopub.status.idle": "2025-05-01T22:07:18.955634Z",
     "shell.execute_reply": "2025-05-01T22:07:18.954170Z",
     "shell.execute_reply.started": "2025-05-01T22:07:17.473431Z"
    }
   },
   "outputs": [],
   "source": [
    "ACOLITE_PATH = \"./acolite\"\n",
    "sys.path.append(ACOLITE_PATH)\n",
    "# Import acolite_run\n",
    "from acolite.acolite.acolite_run import acolite_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:07:18.960551Z",
     "iopub.status.busy": "2025-05-01T22:07:18.959262Z",
     "iopub.status.idle": "2025-05-01T22:07:18.969019Z",
     "shell.execute_reply": "2025-05-01T22:07:18.967330Z",
     "shell.execute_reply.started": "2025-05-01T22:07:18.960514Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_utm_limit(utm_x, utm_y,utm_crs, box_meters, pixel_size=10):\n",
    "    \"\"\"Compute WGS84 limit from UTM center for box_meters x box_meters.\"\"\"\n",
    "    #utm_crs = \"EPSG:32631\"  # UTM 31N\n",
    "    wgs84_crs = \"EPSG:4326\"\n",
    "    utm_to_wgs = Transformer.from_crs(utm_crs, wgs84_crs, always_xy=True)\n",
    "    \n",
    "    half_box = box_meters / 2\n",
    "    box_left = utm_x - half_box\n",
    "    box_right = utm_x + half_box\n",
    "    box_bottom = utm_y - half_box\n",
    "    box_top = utm_y + half_box\n",
    "    \n",
    "    lon_min, lat_min = utm_to_wgs.transform(box_left, box_bottom)\n",
    "    lon_max, lat_max = utm_to_wgs.transform(box_right, box_top)\n",
    "    limit = [lat_min, lon_min, lat_max, lon_max]\n",
    "    \n",
    "    print(f\"UTM box: left={box_left:.1f}, bottom={box_bottom:.1f}, right={box_right:.1f}, top={box_top:.1f}\")\n",
    "    print(f\"WGS84 limit: {limit}\")\n",
    "    return limit, (box_left, box_bottom, box_right, box_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:07:18.970547Z",
     "iopub.status.busy": "2025-05-01T22:07:18.970142Z",
     "iopub.status.idle": "2025-05-01T22:07:19.162359Z",
     "shell.execute_reply": "2025-05-01T22:07:19.161209Z",
     "shell.execute_reply.started": "2025-05-01T22:07:18.970514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S2A_MSIL1C_20150814T094006_N0500_R036_T33SXC_20231010T034522\n",
      "S2A_MSIL1C_20150923T094016_N0500_R036_T33SXC_20231028T021114\n",
      "S2A_MSIL1C_20151115T095252_N0500_R079_T33SXC_20231021T074400\n",
      "S2A_MSIL1C_20161126T094332_N0500_R036_T33SXC_20231025T160709\n",
      "S2A_MSIL1C_20181007T094031_N0500_R036_T33SXC_20230727T233522\n",
      "S2A_MSIL1C_20181010T095031_N0500_R079_T33SXC_20230729T115020\n",
      "S2A_MSIL1C_20181017T094011_N0500_R036_T33SXC_20230729T163213\n",
      "S2A_MSIL1C_20181027T094101_N0500_R036_T33SXC_20230817T053449\n",
      "S2A_MSIL1C_20181106T094201_N0500_R036_T33SXC_20230727T210154\n",
      "S2A_MSIL1C_20181109T095221_N0500_R079_T33SXC_20230813T172922\n",
      "S2A_MSIL1C_20200110T094351_N0500_R036_T33SXC_20230603T093842\n",
      "S2A_MSIL1C_20200409T094031_N0500_R036_T33SXC_20230426T035222\n",
      "S2A_MSIL1C_20201228T095421_N0500_R079_T33SXC_20230401T085523\n"
     ]
    }
   ],
   "source": [
    "! ls /kaggle/input/litter-windrows-batch-cala\n",
    "safe_files_dir ='/kaggle/input/litter-windrows-batch-cala'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:07:19.163709Z",
     "iopub.status.busy": "2025-05-01T22:07:19.163390Z",
     "iopub.status.idle": "2025-05-01T22:07:19.170033Z",
     "shell.execute_reply": "2025-05-01T22:07:19.168328Z",
     "shell.execute_reply.started": "2025-05-01T22:07:19.163677Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset_tiles(safe_files_dir):\n",
    "    tiles = os.listdir(safe_files_dir)\n",
    "    tiles = [str(t)+'.SAFE' for t in tiles]\n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:07:19.172121Z",
     "iopub.status.busy": "2025-05-01T22:07:19.171522Z",
     "iopub.status.idle": "2025-05-01T22:07:19.196555Z",
     "shell.execute_reply": "2025-05-01T22:07:19.195588Z",
     "shell.execute_reply.started": "2025-05-01T22:07:19.172082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S2A_MSIL1C_20201228T095421_N0500_R079_T33SXC_20230401T085523.SAFE',\n",
       " 'S2A_MSIL1C_20151115T095252_N0500_R079_T33SXC_20231021T074400.SAFE',\n",
       " 'S2A_MSIL1C_20181027T094101_N0500_R036_T33SXC_20230817T053449.SAFE',\n",
       " 'S2A_MSIL1C_20200409T094031_N0500_R036_T33SXC_20230426T035222.SAFE',\n",
       " 'S2A_MSIL1C_20181109T095221_N0500_R079_T33SXC_20230813T172922.SAFE',\n",
       " 'S2A_MSIL1C_20181017T094011_N0500_R036_T33SXC_20230729T163213.SAFE',\n",
       " 'S2A_MSIL1C_20150814T094006_N0500_R036_T33SXC_20231010T034522.SAFE',\n",
       " 'S2A_MSIL1C_20181007T094031_N0500_R036_T33SXC_20230727T233522.SAFE',\n",
       " 'S2A_MSIL1C_20150923T094016_N0500_R036_T33SXC_20231028T021114.SAFE',\n",
       " 'S2A_MSIL1C_20161126T094332_N0500_R036_T33SXC_20231025T160709.SAFE',\n",
       " 'S2A_MSIL1C_20200110T094351_N0500_R036_T33SXC_20230603T093842.SAFE',\n",
       " 'S2A_MSIL1C_20181010T095031_N0500_R079_T33SXC_20230729T115020.SAFE',\n",
       " 'S2A_MSIL1C_20181106T094201_N0500_R036_T33SXC_20230727T210154.SAFE']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the tiles in the dataset\n",
    "#safe_files_dir = '/kaggle/input/litter-windrows-batch-cala'\n",
    "# tiles = os.listdir(safe_files_dir)\n",
    "# tiles = [str(t)+'.SAFE' for t in tiles]\n",
    "tiles = get_dataset_tiles(safe_files_dir)\n",
    "tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:07:19.198136Z",
     "iopub.status.busy": "2025-05-01T22:07:19.197768Z",
     "iopub.status.idle": "2025-05-01T22:07:19.219354Z",
     "shell.execute_reply": "2025-05-01T22:07:19.218303Z",
     "shell.execute_reply.started": "2025-05-01T22:07:19.198106Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_crs_and_bounds(safe_file):\n",
    "    #granule_path = os.path.join(os.path.join(safe_files_dir, tiles[0]), tiles[0]+'.SAFE/GRANULE/')\n",
    "    granule_path = os.path.join(safe_file, 'GRANULE/')\n",
    "    #print(granule_path)\n",
    "    granule_subdirs = os.listdir(granule_path)  \n",
    "    img_data_path = os.path.join(granule_path, os.path.join(granule_subdirs[0]),'IMG_DATA/')\n",
    "    jpg2_files = os.listdir(img_data_path)\n",
    "    #print('jpg2_files')\n",
    "    #print(jpg2_files)\n",
    "    B02_files = [f  for f in jpg2_files if f.endswith('B02.jp2')]\n",
    "    #print('B02_files')\n",
    "    #print(B02_files)\n",
    "    band_path = os.path.join(img_data_path, B02_files[0])\n",
    "    print(band_path)\n",
    "    with rasterio.open(band_path) as src:\n",
    "        print(f\"CRS: {src.crs}\")  # Should be EPSG:32631\n",
    "        print(f\"Bounds: {src.bounds}\")  # Exact UTM coordinates\n",
    "        data = src.read(1)\n",
    "        print(f\"B02 Valid Pixels: {np.sum(data > 0)} / {data.size}\")\n",
    "    return src.crs, src.bounds, src.transform, src.res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:07:19.220728Z",
     "iopub.status.busy": "2025-05-01T22:07:19.220400Z",
     "iopub.status.idle": "2025-05-01T22:07:19.247423Z",
     "shell.execute_reply": "2025-05-01T22:07:19.246010Z",
     "shell.execute_reply.started": "2025-05-01T22:07:19.220700Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_tiles(files):\n",
    "    \"\"\"\n",
    "    Given the list of the parquet annotation file, builds the list of all the \n",
    "    annotated tiles.\n",
    "    \"\"\"\n",
    "    df = None\n",
    "    # Process 10 files per batch\n",
    "    batch_size = 10\n",
    "    tiles = []\n",
    "    for i in range(0, len(files), batch_size):\n",
    "        batch_files = files[i:i + batch_size]\n",
    "        \n",
    "        # Read batch with Dask (lazy loading)\n",
    "        ddf = dd.read_parquet(batch_files, engine='pyarrow')\n",
    "        \n",
    "        # Compute to pandas (triggers parallel read)\n",
    "        batch_df = ddf.compute()  # Now contains data from 10 files\n",
    "        tiles.append(batch_df['s2_product'].unique())\n",
    "       \n",
    "        print(f\"Batch {i//batch_size + 1}: {len(batch_df)} rows (from {len(batch_files)} files)\")\n",
    "    \n",
    "    tiles = np.unique(np.hstack(tiles))\n",
    "    tiles = [t.decode('utf-8') for t in tiles]\n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:07:19.249278Z",
     "iopub.status.busy": "2025-05-01T22:07:19.248925Z",
     "iopub.status.idle": "2025-05-01T22:07:19.389259Z",
     "shell.execute_reply": "2025-05-01T22:07:19.388074Z",
     "shell.execute_reply.started": "2025-05-01T22:07:19.249248Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_jp2_ref_file(safe_file):\n",
    "    granule_path = os.path.join(safe_file, 'GRANULE/')\n",
    "    granule_subdirs = os.listdir(granule_path)  \n",
    "    img_data_path = os.path.join(granule_path, os.path.join(granule_subdirs[0]),'IMG_DATA/')\n",
    "    jpg2_files = os.listdir(img_data_path)\n",
    "    B02_files = [f  for f in jpg2_files if f.endswith('B02.jp2')]\n",
    "    if len(B02_files) == 0:\n",
    "        raise ValueError(f'No B2 band jpg2 file retrieve in the safe file {safe_file}')\n",
    "    else :\n",
    "        return os.path.join(img_data_path, B02_files[0])\n",
    "        \n",
    "def check_for_invalid_data(file_path, bbox):\n",
    "    with rasterio.open(file_path) as src:\n",
    "        # Read the window\n",
    "        window = src.window(*bbox)\n",
    "        data = src.read(1, window=window)\n",
    "    \n",
    "    # Sentinel-2 often uses 0 for invalid pixels\n",
    "    invalid_mask = (data == 0)\n",
    "    \n",
    "    # Or sometimes very high values (like 65535 for uint16)\n",
    "    if data.dtype == np.uint16:\n",
    "        invalid_mask = invalid_mask | (data == 65535)\n",
    "    \n",
    "    has_invalid = invalid_mask.any()\n",
    "    \n",
    "    if has_invalid:\n",
    "        invalid_count = invalid_mask.sum()\n",
    "        print(f\"Found {invalid_count} invalid pixels in the bounding box\")\n",
    "    else:\n",
    "        print(\"No invalid pixels found in the bounding box\")\n",
    "    return has_invalid\n",
    "\n",
    "def generate_tile_regions(safe_file_path):\n",
    "    \"\"\"\n",
    "    Returns regions splitting the tile image\n",
    "    in the form of (xmin, ymin, xmax, ymax)\n",
    "    \"\"\"\n",
    "    w = 10980\n",
    "    h = 10980\n",
    "    stepx = 2700\n",
    "    stepy = 2700\n",
    "    sx = 3000\n",
    "    sy = 3000\n",
    "    regions = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            regions.append((i*stepx, j*stepy, min(w, i*stepx + sx), min(h, j*stepy + sy)))\n",
    "    ref_file = find_jp2_ref_file(safe_file_path)\n",
    "    filtered_regions = []\n",
    "   \n",
    "    for r in regions :\n",
    "        invalid = check_for_invalid_data(ref_file, r)\n",
    "        if not invalid:\n",
    "            filtered_regions.append(r)\n",
    "    regions = {i : r for i, r in enumerate(filtered_regions)}\n",
    "    return regions\n",
    "\n",
    "def assign_patches(tile_regions, patches):\n",
    "    splitted_regions = {}\n",
    "    for p in patches: \n",
    "        for idr, r in tile_regions.items():\n",
    "            inside = p[0] >= r[0] and p[1] >= r[1] and p[2] <= r[2] and p[3] <= r[3]\n",
    "            if inside:\n",
    "                if idr  not in splitted_regions:\n",
    "                    splitted_regions[idr] = [p]\n",
    "                else :\n",
    "                    splitted_regions[idr].append(p)\n",
    "                break\n",
    "    return splitted_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:07:19.390875Z",
     "iopub.status.busy": "2025-05-01T22:07:19.390547Z",
     "iopub.status.idle": "2025-05-01T22:07:19.428213Z",
     "shell.execute_reply": "2025-05-01T22:07:19.426784Z",
     "shell.execute_reply.started": "2025-05-01T22:07:19.390817Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_tile_df(target, files):\n",
    "    \"\"\"\n",
    "    Retrieves all annotations for a given target tile.\n",
    "    \"\"\"\n",
    "    df = None\n",
    "    # Process 10 files per batch\n",
    "    batch_size = 10\n",
    "    tg_id_1 = target.split('_')[2]\n",
    "    tg_id_2 = '_'.join(target.split('_')[4:6])\n",
    "    for i in range(0, len(files), batch_size):\n",
    "        batch_files = files[i:i + batch_size]\n",
    "        \n",
    "        # Read batch with Dask (lazy loading)\n",
    "        ddf = dd.read_parquet(batch_files, engine='pyarrow')\n",
    "        \n",
    "        # Compute to pandas (triggers parallel read)\n",
    "        batch_df = ddf.compute()  # Now contains data from 10 files\n",
    "        batch_df[\"s2_product\"] = batch_df[\"s2_product\"].str.decode('utf-8')\n",
    "        batch_df = batch_df[batch_df[\"s2_product\"].str.contains(tg_id_1) & batch_df[\"s2_product\"].str.contains(tg_id_2)]\n",
    "        if not batch_df.empty:\n",
    "            if df is None:\n",
    "                df = batch_df.copy()\n",
    "            else:\n",
    "                df = pd.concat([df, batch_df], ignore_index=True)\n",
    "        print(f\"Batch {i//batch_size + 1}: {len(batch_df)} rows (from {len(batch_files)} files)\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:07:19.430497Z",
     "iopub.status.busy": "2025-05-01T22:07:19.429772Z",
     "iopub.status.idle": "2025-05-01T22:07:19.463379Z",
     "shell.execute_reply": "2025-05-01T22:07:19.462295Z",
     "shell.execute_reply.started": "2025-05-01T22:07:19.430462Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_tile_df(target, tile_parquet_dir):\n",
    "    fpathname = os.path.join(tile_parquet_dir, f'LWR_{target[:-5]}.parquet')\n",
    "    ddf = dd.read_parquet(fpathname, engine='pyarrow')\n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:07:19.464823Z",
     "iopub.status.busy": "2025-05-01T22:07:19.464522Z",
     "iopub.status.idle": "2025-05-01T22:07:19.501592Z",
     "shell.execute_reply": "2025-05-01T22:07:19.500398Z",
     "shell.execute_reply.started": "2025-05-01T22:07:19.464793Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_tiles_parquet_annotations(tiles, files):\n",
    "    \"\"\"\n",
    "    Retrieves all annotations for a given target tile.\n",
    "    \"\"\"\n",
    "    for target in tiles:\n",
    "        df = None\n",
    "        # Process 10 files per batch\n",
    "        batch_size = 10\n",
    "        tg_id_1 = target.split('_')[2]\n",
    "        tg_id_2 = '_'.join(target.split('_')[4:6])\n",
    "        for i in range(0, len(files), batch_size):\n",
    "            batch_files = files[i:i + batch_size]\n",
    "            \n",
    "            # Read batch with Dask (lazy loading)\n",
    "            ddf = dd.read_parquet(batch_files, engine='pyarrow')\n",
    "            \n",
    "            # Compute to pandas (triggers parallel read)\n",
    "            batch_df = ddf.compute()  # Now contains data from 10 files\n",
    "            batch_df[\"s2_product\"] = batch_df[\"s2_product\"].str.decode('utf-8')\n",
    "            batch_df = batch_df[batch_df[\"s2_product\"].str.contains(tg_id_1) & batch_df[\"s2_product\"].str.contains(tg_id_2)  &\n",
    "    (~batch_df[\"pixel_x\"].isna()) &\n",
    "    (~batch_df[\"pixel_y\"].isna()) &\n",
    "    (batch_df[\"pixel_x\"] != -999) &\n",
    "    (batch_df[\"pixel_y\"] != -999) ]\n",
    "            if not batch_df.empty:\n",
    "                if df is None:\n",
    "                    df = batch_df.copy()\n",
    "                else:\n",
    "                    df = pd.concat([df, batch_df], ignore_index=True)\n",
    "            print(f\"Batch {i//batch_size + 1}: {len(batch_df)} rows (from {len(batch_files)} files)\")\n",
    "        # Save to Parquet\n",
    "        df.to_parquet(f'LWR_{target[:-5]}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:07:19.504991Z",
     "iopub.status.busy": "2025-05-01T22:07:19.504500Z",
     "iopub.status.idle": "2025-05-01T22:07:19.541600Z",
     "shell.execute_reply": "2025-05-01T22:07:19.540497Z",
     "shell.execute_reply.started": "2025-05-01T22:07:19.504962Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_relative_path(target_path, start_path):\n",
    "    \"\"\"\n",
    "    Returns the relative path from start_path to target_path\n",
    "    \"\"\"\n",
    "    return os.path.relpath(target_path, start_path)\n",
    "\n",
    "\n",
    "def get_all_files_recursive(directory):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        #print(files)\n",
    "        for file in files:\n",
    "            full_path = os.path.abspath(os.path.join(root, file))\n",
    "            if os.path.isfile(full_path):  # Check to ensure it's a file\n",
    "                file_paths.append(full_path)\n",
    "    return file_paths\n",
    "\n",
    "def upload_to_dagshub(root_folder, src_folder, dst_folder, batch_id, dagshub_token):\n",
    "    \"\"\"\n",
    "    dst_folder: Must have the format './dest_folder',\n",
    "    where dst_folder is the destination folder for the images\n",
    "    relative to the root of the repository.\n",
    "    \n",
    "    root_folder: The relative path of src_folder with respect to root_folder\n",
    "    will determine the position of the uploaded file relative to the root\n",
    "    of the repository.\n",
    "    \"\"\"\n",
    "\n",
    "    # Defining the Repo & directory\n",
    "    add_app_token(dagshub_token)\n",
    "    repo = Repo(\"elena-andreini\", \"TriesteItalyChapter_PlasticDebrisDetection\")\n",
    "    ds = repo.directory(dst_folder)\n",
    "    files = get_all_files_recursive(src_folder)\n",
    "    print(f'uploading files {files}')\n",
    "    rel_files_paths = [get_relative_path(f, src_folder) for f in files]\n",
    "    print(f'relative files {rel_files_paths}')\n",
    "\n",
    "    for i in rel_files_paths:\n",
    "      ds.add(file=os.path.join(src_folder, i), path=f'./{i}')\n",
    "    \n",
    "    # Commit the changes\n",
    "    ds.commit(f'Adding batch {batch_id}  to {dst_folder} folder', versioning='dvc')\n",
    "    print(f'Uploaded batch {batch_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:07:19.543270Z",
     "iopub.status.busy": "2025-05-01T22:07:19.542788Z",
     "iopub.status.idle": "2025-05-01T22:07:19.575475Z",
     "shell.execute_reply": "2025-05-01T22:07:19.574332Z",
     "shell.execute_reply.started": "2025-05-01T22:07:19.543243Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define settings form map-mapper paper\n",
    "settings = {\n",
    "    # Ensure TIFF export\n",
    "    'l2r_export_geotiff': True,  \n",
    "    # delete .nc once made to geotiff\n",
    "    'l1r_delete_netcdf' : True,\n",
    "    'l2w_delete_netcdf' : True,\n",
    "    'verbosity': 5,  # Detailed logging\n",
    "    's2_target_res': 10,  # 10m resolution\n",
    "    'resampling_method': 'nearest' , # Set to nearest neighbor\n",
    "    'atmospheric_correction_method': 'dark_spectrum',\n",
    "    'dsf_exclude_bands' : ['B9', 'B10'],\n",
    "    # resolves issue with none type see this thread - https://odnature.naturalsciences.be/remsem/acolite-forum/viewtopic.php?t=319\n",
    "    'geometry_type' : 'grids',\n",
    "    #masking\n",
    "    'l2w_mask' : False,\n",
    "    #sunglint\n",
    "    'glint_correction' : True,\n",
    "    'dsf_residual_glint_correction' : True,\n",
    "    'dsf_residual_glint_correction_method' : 'alternative', # index error occuring with default\n",
    "    'dsf_residual_glint_wave_range' : [1500,2400],\n",
    "    'glint_force_band' : None,\n",
    "    'glint_mask_rhos_wave' : 1600,\n",
    "    'glint_mask_rhos_threshold' : 0.11,\n",
    "    'reproject' : False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:07:19.580312Z",
     "iopub.status.busy": "2025-05-01T22:07:19.579976Z",
     "iopub.status.idle": "2025-05-01T22:07:19.623193Z",
     "shell.execute_reply": "2025-05-01T22:07:19.621942Z",
     "shell.execute_reply.started": "2025-05-01T22:07:19.580288Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_tile_mask(tile_df):\n",
    "    \"\"\"\n",
    "    Generate mask for the whole tile.\n",
    "    Based on dhia's code.\n",
    "    Not filtering filaments by box_dims size at the moment\n",
    "    \"\"\"\n",
    "    mask = np.zeros((10980, 10980))\n",
    "    pixels = 0\n",
    "    #pb = tqdm(tile_df[tile_df[\"box_dims\"] == 3].iterrows())\n",
    "    pb = tqdm(tile_df.iterrows())\n",
    "    for index, row in pb:\n",
    "        x, y = row[\"pixel_x\"], row[\"pixel_y\"]\n",
    "        if not np.isnan(x) and not np.isnan(y):\n",
    "            pixels += 1\n",
    "            x, y = int(x), int(y)\n",
    "            mask[x, y] = 1\n",
    "    return mask, pixels\n",
    "    \n",
    "def find_subregions_efficient(binary_image, subregion_size, threshold):\n",
    "    \"\"\"\n",
    "    Finds regions with debris pixels in the whole tile annotation mask.\n",
    "    Using integral images for large binary images.\n",
    "    \"\"\"\n",
    "    h, w = subregion_size\n",
    "    img_h, img_w = binary_image.shape\n",
    "    print(f'input maks shape : {binary_image.shape}')\n",
    "    # Convert to binary and create integral image\n",
    "    binary = (binary_image == 1).astype(np.uint8)\n",
    "    integral = cv2.integral(binary)\n",
    "    \n",
    "    subregions = []\n",
    "    covered = np.zeros_like(binary, dtype=bool)\n",
    "    \n",
    "    for y in range(0, img_h - h + 1, h):\n",
    "        for x in range(0, img_w - w + 1, w):\n",
    "            #if covered[y:y+h, x:x+w].any():\n",
    "            #    continue\n",
    "                \n",
    "            # Calculate sum using integral image\n",
    "            total = integral[y+h, x+w] - integral[y, x+w] - integral[y+h, x] + integral[y, x]\n",
    "            white_fraction = total / (h * w)\n",
    "            #print(f'white_fraction {white_fraction}')\n",
    "            if total > threshold:\n",
    "                subregions.append((x, y,  x + w,  y + h, total))\n",
    "                covered[y:y+h, x:x+w] = True\n",
    "                \n",
    "    return subregions\n",
    "\n",
    "def minimal_1024_cover(small_regions):\n",
    "    \"\"\"\n",
    "    This function could be used to find larger regions around patches\n",
    "    for ACOLITE application.\n",
    "    DSF algorithm could be not completely reliable applied to regions of 2560x2560 m unless\n",
    "    they are very homogeneous\n",
    "    \"\"\"\n",
    "    # Converti le coordinate in celle della griglia 256×256\n",
    "    cells = set((x // 1024, y // 1024) for (y, x, _, _,_) in small_regions)\n",
    "    if not cells:\n",
    "        return []\n",
    "    large_regions = set()\n",
    "    for (i, j) in cells:\n",
    "        large_i = i * 1024\n",
    "        large_j = j * 1024\n",
    "        large_regions.add((large_i, large_j))\n",
    "    return large_regions\n",
    "\n",
    "\n",
    "def upload_to_drive(file_path, drive_folder_id):\n",
    "    \"\"\"Upload file to Google Drive.\"\"\"\n",
    "    output = gdown.upload(file_path, parent_id=drive_folder_id, quiet=False)\n",
    "    print(f\"Uploaded {file_path} to Drive folder {drive_folder_id}\")\n",
    "    os.remove(file_path)  # Clear Kaggle disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:09:10.184440Z",
     "iopub.status.busy": "2025-05-01T22:09:10.184050Z",
     "iopub.status.idle": "2025-05-01T22:09:10.192089Z",
     "shell.execute_reply": "2025-05-01T22:09:10.190296Z",
     "shell.execute_reply.started": "2025-05-01T22:09:10.184416Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate_image(image):\n",
    "    # Define invalid values (NaN, Inf, or negative)\n",
    "    invalid_mask = np.isnan(image) | np.isinf(image) | (image < 0)\n",
    "    total_pixels = image.size\n",
    "    invalid_count = invalid_mask.sum()\n",
    "    invalid_percentage = (invalid_count / total_pixels) * 100\n",
    "    print(f\"Percentage of invalid values (NaN, Inf, negative) across all bands: {invalid_percentage:.2f}%\")\n",
    "    if invalid_percentage > 10:\n",
    "        print('skipping region, too many invalid pixels ')\n",
    "        return False\n",
    "    else :\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:09:10.429016Z",
     "iopub.status.busy": "2025-05-01T22:09:10.428597Z",
     "iopub.status.idle": "2025-05-01T22:09:10.445729Z",
     "shell.execute_reply": "2025-05-01T22:09:10.444518Z",
     "shell.execute_reply.started": "2025-05-01T22:09:10.428981Z"
    }
   },
   "outputs": [],
   "source": [
    "#### Stacking \n",
    "\n",
    "\n",
    "def stack_bands(acolite_output_dir, stacked_dir, tile):\n",
    "    # Mapping wawelength to band names\n",
    "    wl_to_band  ={\n",
    "        443 : 'B1',\n",
    "        492 : 'B2',\n",
    "        560 : 'B3',\n",
    "        665 : 'B4',\n",
    "        704 : 'B5',\n",
    "        740 : 'B6',\n",
    "        783 : 'B7',\n",
    "        833 : 'B8',\n",
    "        865 : 'B8A',\n",
    "        1614 : 'B11',\n",
    "        2202 : 'B12'\n",
    "    }\n",
    "    rhos_files = [f for f in os.listdir(acolite_output_dir) if 'rhos' in f and '.tif' in f]\n",
    "    rhos_files = sorted(rhos_files, key=lambda x: int(x.split('_')[-1][:-4]))\n",
    "    band_files = [(wl_to_band[int(f.split('_')[-1][:-4])], f) for i,f in enumerate(rhos_files) ]\n",
    "\n",
    "    # Verify all files exist\n",
    "    for band, filename in band_files:\n",
    "        if not os.path.exists(os.path.join(acolite_output_dir, filename)):\n",
    "            raise FileNotFoundError(f\"Missing file: {filename}\")\n",
    "\n",
    "    # Reference band (use B4 for metadata, since all bands are 10m)\n",
    "    reference_band = 'B4'\n",
    "    reference_file = os.path.join(acolite_output_dir, [f for b, f in band_files if b == reference_band][0])\n",
    "\n",
    "    # Open the reference GeoTIFF to get metadata\n",
    "    with rasterio.open(reference_file) as ref:\n",
    "        ref_profile = ref.profile\n",
    "        ref_transform = ref.transform\n",
    "        ref_crs = ref.crs\n",
    "        ref_width = ref.width\n",
    "        ref_height = ref.height\n",
    "        ref_resolution = ref.res  # Should be (10.0, 10.0)\n",
    "\n",
    "    # Initialize an array to store all bands\n",
    "    stacked_data = np.zeros((len(band_files), ref_height, ref_width), dtype=np.float32)\n",
    "    \n",
    "    # Read each band (no resampling needed, all are 10m)\n",
    "    for i, (band, filename) in enumerate(band_files):\n",
    "        file_path = os.path.join(acolite_output_dir, filename)\n",
    "        with rasterio.open(file_path) as src:\n",
    "            # Verify resolution matches\n",
    "            if src.res != ref_resolution:\n",
    "                raise ValueError(f\"Resolution mismatch in {filename}: expected {ref_resolution}, got {src.res}\")\n",
    "            # Verify dimensions match\n",
    "            if src.width != ref_width or src.height != ref_height:\n",
    "                raise ValueError(f\"Dimensions mismatch in {filename}: expected {ref_width}x{ref_height}, got {src.width}x{src.height}\")\n",
    "            # Read the band\n",
    "            data = src.read(1, out_dtype=np.float32)  # Single band, float32\n",
    "            stacked_data[i] = data\n",
    "\n",
    "    # Update the profile for the stacked GeoTIFF\n",
    "    stacked_profile = ref_profile.copy()\n",
    "    stacked_profile.update({\n",
    "        'count': len(band_files),  # 11 bands\n",
    "        'dtype': np.float32,  # For reflectance\n",
    "        'transform': ref_transform,\n",
    "        'crs': ref_crs,\n",
    "        'width': ref_width,\n",
    "        'height': ref_height,\n",
    "        'nodata': -999  # Optional: Set nodata value (adjust if ACOLITE uses NaN)\n",
    "    })\n",
    "    # Determine patch name similar to MARIDA : S2_DATE_TILE_REGION in folder S2_DATE_TILE\n",
    "    stacked_tif_fname =  'S2_'+('_'.join(tile.split('_')[2:-3]))+'_stacked.tiff'\n",
    "    stacked_tif = os.path.join(stacked_dir, stacked_tif_fname)\n",
    "    # Save the stacked GeoTIFF\n",
    "    with rasterio.open(stacked_tif, 'w', **stacked_profile) as dst:\n",
    "        dst.write(stacked_data)\n",
    "        # Set band descriptions in the usual order\n",
    "        for i, (band, _) in enumerate(band_files, 1):\n",
    "            dst.set_band_description(i, band)\n",
    "    \n",
    "    print(f\"Stacked GeoTIFF saved to: {stacked_tif}\")\n",
    "\n",
    "    # Verify the stacked GeoTIFF\n",
    "    with rasterio.open(stacked_tif) as stacked:\n",
    "        print(f\"Stacked GeoTIFF:\")\n",
    "        print(f\"Bounds: {stacked.bounds}\")\n",
    "        print(f\"Dimensions: {stacked.width} columns, {stacked.height} rows\")\n",
    "        print(f\"Resolution: {stacked.res}\")\n",
    "        print(f\"CRS: {stacked.crs}\")\n",
    "        print(f\"Number of bands: {stacked.count}\")\n",
    "        print(f\"Band descriptions: {stacked.descriptions}\")\n",
    "    valid = validate_image(stacked_data)\n",
    "    return stacked_tif, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:22:03.838789Z",
     "iopub.status.busy": "2025-05-01T22:22:03.836124Z",
     "iopub.status.idle": "2025-05-01T22:22:03.857719Z",
     "shell.execute_reply": "2025-05-01T22:22:03.856250Z",
     "shell.execute_reply.started": "2025-05-01T22:22:03.838732Z"
    }
   },
   "outputs": [],
   "source": [
    "######## Cropping\n",
    "\n",
    "\n",
    "# Paths to the original GeoTIFF, stacked GeoTIFF, and output\n",
    "def crop_stacked_file(stacked_tif, cropped_tif, utm_bounds, crs, tile_res):\n",
    "    \n",
    "    # Computing metadata for the original patch (before ACOLITE correction)\n",
    "    ##########################################################\n",
    "    with rasterio.open(stacked_tif) as src:\n",
    "        # Calculate the window to read\n",
    "        window = from_bounds(*utm_bounds, transform=src.transform)\n",
    "\n",
    "        # Read the data in this window\n",
    "        data = src.read(window=window)\n",
    "    \n",
    "        # Update the transform for the new cropped image\n",
    "        transform = rasterio.windows.transform(window, src.transform)\n",
    "    \n",
    "        # Get band descriptions to preserve metadata\n",
    "        band_descriptions = src.descriptions\n",
    "        \n",
    "        # Write the cropped image\n",
    "        with rasterio.open(\n",
    "            cropped_tif,\n",
    "            'w',\n",
    "            driver='GTiff',\n",
    "            height=window.height,\n",
    "            width=window.width,\n",
    "            count=src.count,\n",
    "            dtype=data.dtype,\n",
    "            crs=src.crs,\n",
    "            transform=transform,\n",
    "        ) as dst:\n",
    "            # Preserve band descriptions (e.g., B1, B2, ...)\n",
    "            for i, desc in enumerate(band_descriptions, 1):\n",
    "                if desc:  # Only set if description exists\n",
    "                    dst.set_band_description(i, desc)\n",
    "            dst.write(data)\n",
    "    \n",
    "    print(f\"Cropped GeoTIFF saved to: {cropped_tif}\")\n",
    "\n",
    "    # Verify the cropped GeoTIFF\n",
    "    with rasterio.open(cropped_tif) as cropped:\n",
    "        print(f\"Cropped GeoTIFF:\")\n",
    "        print(f\"Bounds: {cropped.bounds}\")\n",
    "        print(f\"Dimensions: {cropped.width} columns, {cropped.height} rows\")\n",
    "        print(f\"Resolution: {cropped.res}\")\n",
    "        print(f\"CRS: {cropped.crs}\")\n",
    "        print(f\"Number of bands: {cropped.count}\")\n",
    "        print(f\"Band descriptions: {cropped.descriptions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:09:10.978783Z",
     "iopub.status.busy": "2025-05-01T22:09:10.978501Z",
     "iopub.status.idle": "2025-05-01T22:09:10.983425Z",
     "shell.execute_reply": "2025-05-01T22:09:10.982219Z",
     "shell.execute_reply.started": "2025-05-01T22:09:10.978765Z"
    }
   },
   "outputs": [],
   "source": [
    "##### Run if you want to extract all tiles from the LW annotation file\n",
    "#%%time\n",
    "#all_tiles = get_all_tiles(annotations_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:09:11.157370Z",
     "iopub.status.busy": "2025-05-01T22:09:11.156908Z",
     "iopub.status.idle": "2025-05-01T22:09:11.208714Z",
     "shell.execute_reply": "2025-05-01T22:09:11.207206Z",
     "shell.execute_reply.started": "2025-05-01T22:09:11.157340Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_tile(target, safe_files_dir, annotations_files, output_dir, remote_dataset_location = './LWC_DataSet/test_data', batch_id='', clean_patches = True, dagshub_token=''):\n",
    "    acolite_output_dir = os.path.join(output_dir, 'acolite_output/')\n",
    "    stacked_dir = os.path.join(output_dir, 'stacked/')\n",
    "    final_dir = os.path.join(output_dir, 'patches/')\n",
    "    os.makedirs(acolite_output_dir, exist_ok=True)\n",
    "    os.makedirs(stacked_dir, exist_ok=True)\n",
    "    if clean_patches : \n",
    "        shutil.rmtree(acolite_output_dir, ignore_errors=True)\n",
    "    os.makedirs(final_dir, exist_ok=True)\n",
    "    print(f'building annotations dataframe for tile {target}')\n",
    "    tile_df = build_tile_df(target, annotations_files)\n",
    "    #tile_df = read_tile_df(target, '/kaggle/working')\n",
    "    mask, debris_pixels_count = generate_tile_mask(tile_df)\n",
    "    print(f'tile mask shape {mask.shape}')\n",
    "    if debris_pixels_count == 0:\n",
    "        print(f'no valid debris pixels in tile {target}')\n",
    "        return False\n",
    "    patches = find_subregions_efficient(mask, (256, 256), 25) #### CHANGED THRESHOLD TO 25 HERE TO EXLUDE MINIMAL PLASTIC PATCHES \n",
    "    print(f'patches with marine debris : {patches}')\n",
    "    target_file_path = os.path.join(os.path.join(safe_files_dir, target[:-5]), target)\n",
    "    crs, bounds, tile_transform, tile_res = extract_crs_and_bounds(target_file_path)       \n",
    "    print(f'crs : {crs}')\n",
    "    print(f'bounds : {bounds}')\n",
    "    res = tile_res[0] #sentinel 2 max resolution\n",
    "    tile_regions = generate_tile_regions(target_file_path)\n",
    "    patches_per_region = assign_patches(tile_regions, patches)\n",
    "    print(f'patches per region {patches_per_region}')\n",
    "    for i, ppr in patches_per_region.items(): #iterates over regions containing patches\n",
    "        shutil.rmtree(stacked_dir, ignore_errors=True)\n",
    "        os.makedirs(stacked_dir, exist_ok=True)\n",
    "        current_region = tile_regions[i]\n",
    "        current_center = ((current_region[0] + current_region[2])/2 * res + bounds.left,\n",
    "        (current_region[1] + current_region[3])/2 * (-res) + bounds.top)\n",
    "        #subregion_centers = [((int(p[1] + p[3])/2) * res + bounds.left, int((p[0] + p[2])/2) * res + bounds.bottom) for p in patches]\n",
    "        print(f'region center : {current_center}')\n",
    "        #subregion_size = (256 * res, 256 * res)\n",
    "        current_region_size = ((current_region[2] - current_region[0]) * res, \n",
    "                               (current_region[3] - current_region[1]) * res )\n",
    "        try:\n",
    "            # Check disk usage\n",
    "            print(\"Disk usage before run:\")\n",
    "            !du -sh /kaggle/input/*\n",
    "            !df -h /kaggle/tmp /kaggle/working\n",
    "        \n",
    "            # Clear output directory\n",
    "            shutil.rmtree(acolite_output_dir, ignore_errors=True)\n",
    "            os.makedirs(acolite_output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "        # Process each subregion\n",
    "        #for i, (subregion_utm_x, subregion_utm_y) in enumerate(subregion_centers):\n",
    "            # Clear output directory\n",
    "            subregion_utm_x, subregion_utm_y = current_center\n",
    "            print(f'Cleaning previous acolite output')\n",
    "            shutil.rmtree(acolite_output_dir, ignore_errors=True)\n",
    "            os.makedirs(acolite_output_dir, exist_ok=True)\n",
    "            print(f\"\\nProcessing subregion {i+1}/{len(tile_regions)} centered at UTM ({subregion_utm_x}, {subregion_utm_y})\")\n",
    "    \n",
    "            # Compute UTM-based limit\n",
    "            limit, utm_box = get_utm_limit(subregion_utm_x, subregion_utm_y, crs, current_region_size[0]) #subregion_size[0])\n",
    "            print(f'setting limit : {limit}')\n",
    "            settings['limit'] = limit\n",
    "            print(f\"Settings for subregion {i+1}: {settings}\")\n",
    "            output_files = acolite_run(settings=settings, inputfile=target_file_path, output=acolite_output_dir)\n",
    "            print(f\"Generated output files {output_files}\")\n",
    "            print('stacking bands')\n",
    "            stacked_tiff, valid = stack_bands(acolite_output_dir, stacked_dir, target)\n",
    "            if not valid : \n",
    "                print(f'skipping region {current_region} : too many invalid pixels')\n",
    "                continue\n",
    "            patch_dir =  os.path.join(final_dir, '_'.join((stacked_tiff.split('/')[-1]).split('_')[:-1]))# '_'.join(stacked_tiff.split('_')[:-1])\n",
    "            print(f'patch dir {patch_dir}')\n",
    "            for p in ppr:\n",
    "                print(f'cropping  patch {p}')\n",
    "                patch_utm_bound = (p[0] * res + bounds.left, (p[3]) * (-res) + bounds.top,  ## CHANGED -1 removed\n",
    "                                   p[2] * res + bounds.left,  p[1] * (-res) + bounds.top)\n",
    "                patch_name_prefix = patch_dir.split('/')[-1]\n",
    "                patch_name = f'{patch_name_prefix}_{int(patch_utm_bound[0])}_{int(patch_utm_bound[1])}.tif'  ## CHANGED one f removed from .tiff\n",
    "                patch_pathname = os.path.join(patch_dir, patch_name)\n",
    "                os.makedirs(patch_dir, exist_ok=True)\n",
    "                print(f'patch pathname {patch_pathname}')\n",
    "                crop_stacked_file(stacked_tiff, os.path.join(patch_dir, patch_pathname), patch_utm_bound, crs, tile_res)\n",
    "                patch_mask = mask[p[1] : p[3], p[0] : p[2]]\n",
    "                print(f'patch mask shape {patch_mask.shape}')\n",
    "                try:\n",
    "                    cv2.imwrite(os.path.join(patch_dir, f'{patch_pathname[:-4]}_cl.tif'), patch_mask)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error writing mask: {e}\")\n",
    "            # Insert Sanity Check for ACOLITE output ? \n",
    "            \n",
    "            # Upload to DagsHub\n",
    "            if dagshub_token:\n",
    "                upload_to_dagshub(output_dir, final_dir, remote_dataset_location, batch_id, dagshub_token)\n",
    "    \n",
    "    \n",
    "\n",
    "            # Final disk usage\n",
    "            print(\"\\nDisk usage after run:\")\n",
    "            ! du -sh /kaggle/input/*\n",
    "            ! df -h /kaggle/tmp /kaggle/working\n",
    "      \n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Exception captured: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:09:11.288578Z",
     "iopub.status.busy": "2025-05-01T22:09:11.288227Z",
     "iopub.status.idle": "2025-05-01T22:09:11.315938Z",
     "shell.execute_reply": "2025-05-01T22:09:11.314236Z",
     "shell.execute_reply.started": "2025-05-01T22:09:11.288554Z"
    }
   },
   "outputs": [],
   "source": [
    "### Setting the directory of the annotations file split and converted to parquet format\n",
    "parquet_annotations_dir = '/kaggle/input/lw-parquet/kaggle/working'\n",
    "annotations_files = [os.path.join(parquet_annotations_dir, f) for f in os.listdir(parquet_annotations_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:09:21.272770Z",
     "iopub.status.busy": "2025-05-01T22:09:21.272460Z",
     "iopub.status.idle": "2025-05-01T22:09:21.278644Z",
     "shell.execute_reply": "2025-05-01T22:09:21.277262Z",
     "shell.execute_reply.started": "2025-05-01T22:09:21.272750Z"
    }
   },
   "outputs": [],
   "source": [
    "#build_tiles_parquet_annotations([tiles[5]], annotations_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:09:21.769046Z",
     "iopub.status.busy": "2025-05-01T22:09:21.768651Z",
     "iopub.status.idle": "2025-05-01T22:09:21.774430Z",
     "shell.execute_reply": "2025-05-01T22:09:21.773405Z",
     "shell.execute_reply.started": "2025-05-01T22:09:21.769019Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_batch(tiles, safe_files_dir, annotations_files, batch_id='', dagshub_token=''):\n",
    "    \"\"\"\n",
    "    tiles :  list of tiles (SAFE files) in batch batch_id\n",
    "    \"\"\"\n",
    "    clean_patches = len(dagshub_token) != 0\n",
    "    for target in tiles:\n",
    "        process_tile(target, safe_files_dir, annotations_files, '/kaggle/working', batch_id, clean_patches =clean_patches, dagshub_token=dagshub_token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T22:22:19.707313Z",
     "iopub.status.busy": "2025-05-01T22:22:19.706580Z",
     "iopub.status.idle": "2025-05-01T22:27:27.407566Z",
     "shell.execute_reply": "2025-05-01T22:27:27.406045Z",
     "shell.execute_reply.started": "2025-05-01T22:22:19.707282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building annotations dataframe for tile S2A_MSIL1C_20181017T094011_N0500_R036_T33SXC_20230729T163213.SAFE\n",
      "Batch 1: 0 rows (from 10 files)\n",
      "Batch 2: 0 rows (from 10 files)\n",
      "Batch 3: 0 rows (from 10 files)\n",
      "Batch 4: 0 rows (from 10 files)\n",
      "Batch 5: 30756 rows (from 10 files)\n",
      "Batch 6: 0 rows (from 10 files)\n",
      "Batch 7: 0 rows (from 10 files)\n",
      "Batch 8: 0 rows (from 10 files)\n",
      "Batch 9: 0 rows (from 10 files)\n",
      "Batch 10: 0 rows (from 10 files)\n",
      "Batch 11: 0 rows (from 10 files)\n",
      "Batch 12: 0 rows (from 10 files)\n",
      "Batch 13: 0 rows (from 10 files)\n",
      "Batch 14: 0 rows (from 10 files)\n",
      "Batch 15: 0 rows (from 8 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30756it [00:01, 19499.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tile mask shape (10980, 10980)\n",
      "input maks shape : (10980, 10980)\n",
      "patches with marine debris : [(1024, 7424, 1280, 7680, 42)]\n",
      "/kaggle/input/litter-windrows-batch-cala/S2A_MSIL1C_20181017T094011_N0500_R036_T33SXC_20230729T163213/S2A_MSIL1C_20181017T094011_N0500_R036_T33SXC_20230729T163213.SAFE/GRANULE/L1C_T33SXC_A017336_20181017T094338/IMG_DATA/T33SXC_20181017T094011_B02.jp2\n",
      "CRS: EPSG:32633\n",
      "Bounds: BoundingBox(left=600000.0, bottom=4190220.0, right=709800.0, top=4300020.0)\n",
      "B02 Valid Pixels: 120560400 / 120560400\n",
      "crs : EPSG:32633\n",
      "bounds : BoundingBox(left=600000.0, bottom=4190220.0, right=709800.0, top=4300020.0)\n",
      "No invalid pixels found in the bounding box\n",
      "No invalid pixels found in the bounding box\n",
      "No invalid pixels found in the bounding box\n",
      "No invalid pixels found in the bounding box\n",
      "No invalid pixels found in the bounding box\n",
      "No invalid pixels found in the bounding box\n",
      "No invalid pixels found in the bounding box\n",
      "No invalid pixels found in the bounding box\n",
      "No invalid pixels found in the bounding box\n",
      "No invalid pixels found in the bounding box\n",
      "No invalid pixels found in the bounding box\n",
      "No invalid pixels found in the bounding box\n",
      "No invalid pixels found in the bounding box\n",
      "No invalid pixels found in the bounding box\n",
      "No invalid pixels found in the bounding box\n",
      "No invalid pixels found in the bounding box\n",
      "patches per region {2: [(1024, 7424, 1280, 7680, 42)]}\n",
      "region center : (615000.0, 4231020.0)\n",
      "Disk usage before run:\n",
      "5.7G\t/kaggle/input/litter-windrows-batch-cala\n",
      "68M\t/kaggle/input/lw-parquet\n",
      "df: /kaggle/tmp: No such file or directory\n",
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "/dev/loop1       20G  3.1G   17G  16% /kaggle/working\n",
      "Cleaning previous acolite output\n",
      "\n",
      "Processing subregion 3/16 centered at UTM (615000.0, 4231020.0)\n",
      "UTM box: left=600000.0, bottom=4216020.0, right=630000.0, top=4246020.0\n",
      "WGS84 limit: [38.086443016258336, 16.140308489372842, 38.35291224176292, 16.487802408581686]\n",
      "setting limit : [38.086443016258336, 16.140308489372842, 38.35291224176292, 16.487802408581686]\n",
      "Settings for subregion 3: {'l2r_export_geotiff': True, 'l1r_delete_netcdf': True, 'l2w_delete_netcdf': True, 'verbosity': 5, 's2_target_res': 10, 'resampling_method': 'nearest', 'atmospheric_correction_method': 'dark_spectrum', 'dsf_exclude_bands': ['B9', 'B10'], 'geometry_type': 'grids', 'l2w_mask': False, 'glint_correction': True, 'dsf_residual_glint_correction': True, 'dsf_residual_glint_correction_method': 'alternative', 'dsf_residual_glint_wave_range': [1500, 2400], 'glint_force_band': None, 'glint_mask_rhos_wave': 1600, 'glint_mask_rhos_threshold': 0.11, 'reproject': False, 'limit': [38.086443016258336, 16.140308489372842, 38.35291224176292, 16.487802408581686]}\n",
      "Running ACOLITE processing - Generic GitHub Clone c2025-05-01T22:06:25\n",
      "Python - linux - 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
      "Platform - Linux 6.6.56+ - x86_64 - #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\n",
      "Run ID - 20250501_222544\n",
      "Identified /kaggle/input/litter-windrows-batch-cala/S2A_MSIL1C_20181017T094011_N0500_R036_T33SXC_20230729T163213/S2A_MSIL1C_20181017T094011_N0500_R036_T33SXC_20230729T163213.SAFE as Sentinel-2 type\n",
      "Starting conversion of 1 scenes\n",
      "Starting conversion of /kaggle/input/litter-windrows-batch-cala/S2A_MSIL1C_20181017T094011_N0500_R036_T33SXC_20230729T163213/S2A_MSIL1C_20181017T094011_N0500_R036_T33SXC_20230729T163213.SAFE\n",
      "Importing metadata from L1C_T33SXC_A017336_20181017T094338\n",
      "Reading per pixel geometry\n",
      "Wrote raa (3000, 3049)\n",
      "Wrote vza (3000, 3049)\n",
      "Wrote sza (3000, 3049)\n",
      "Writing geolocation lon/lat\n",
      "Wrote lon (3000, 3049)\n",
      "Wrote lat (3000, 3049)\n",
      "Converting bands\n",
      "Converting bands: Wrote rhot_443 ((3000, 3049))\n",
      "Converting bands: Wrote rhot_492 ((3000, 3049))\n",
      "Converting bands: Wrote rhot_560 ((3000, 3049))\n",
      "Converting bands: Wrote rhot_665 ((3000, 3049))\n",
      "Converting bands: Wrote rhot_704 ((3000, 3049))\n",
      "Converting bands: Wrote rhot_740 ((3000, 3049))\n",
      "Converting bands: Wrote rhot_783 ((3000, 3049))\n",
      "Converting bands: Wrote rhot_833 ((3000, 3049))\n",
      "Converting bands: Wrote rhot_865 ((3000, 3049))\n",
      "Converting bands: Wrote rhot_945 ((3000, 3049))\n",
      "Converting bands: Wrote rhot_1373 ((3000, 3049))\n",
      "Converting bands: Wrote rhot_1614 ((3000, 3049))\n",
      "Converting bands: Wrote rhot_2202 ((3000, 3049))\n",
      "Conversion took 21.0 seconds\n",
      "Created /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L1R.nc\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L1R_rgb_rhot.png\n",
      "Running acolite for /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L1R.nc\n",
      "Getting ancillary data for 2018-10-17T09:49:59.095443+00:00 16.317E 38.220N\n",
      "Downloading file GMAO_MERRA2.20181017T090000.MET.nc\n",
      "EARTHDATA user name and password required for download of https://oceandata.sci.gsfc.nasa.gov/ob/getfile/GMAO_MERRA2.20181017T090000.MET.nc\n",
      "Finished downloading file GMAO_MERRA2.20181017T090000.MET.nc\n",
      "Downloading file GMAO_MERRA2.20181017T100000.MET.nc\n",
      "EARTHDATA user name and password required for download of https://oceandata.sci.gsfc.nasa.gov/ob/getfile/GMAO_MERRA2.20181017T100000.MET.nc\n",
      "Finished downloading file GMAO_MERRA2.20181017T100000.MET.nc\n",
      "No ozone file found for 2018-10-17T09:49:59.095443+00:00\n",
      "No NCEP files found for 2018-10-17T09:49:59.095443+00:00\n",
      "default uoz: 0.30 uwv: 1.50 pressure: 1013.25\n",
      "current uoz: 0.30 uwv: 1.50 pressure: 1013.25\n",
      "Using DSF atmospheric correction\n",
      "Processing with 30 tiles (5x6 tiles of 600x600 pixels)\n",
      "Writing lon\n",
      "Wrote lon (3000, 3049)\n",
      "Writing lat\n",
      "Wrote lat (3000, 3049)\n",
      "Writing sza\n",
      "Wrote sza (3000, 3049)\n",
      "Writing vza\n",
      "Wrote vza (3000, 3049)\n",
      "Writing raa\n",
      "Wrote raa (3000, 3049)\n",
      "Loading LUTs ['ACOLITE-LUT-202110-MOD1', 'ACOLITE-LUT-202110-MOD2']\n",
      "Loading LUTs took 2.7 s\n",
      "Running AOT estimation for band 1 (rhot_443)\n",
      "S2A_MSI/B1 ACOLITE-LUT-202110-MOD1 took 0.001s (RevLUT)\n",
      "S2A_MSI/B1 ACOLITE-LUT-202110-MOD2 took 0.001s (RevLUT)\n",
      "Running AOT estimation for band 2 (rhot_492)\n",
      "S2A_MSI/B2 ACOLITE-LUT-202110-MOD1 took 0.001s (RevLUT)\n",
      "S2A_MSI/B2 ACOLITE-LUT-202110-MOD2 took 0.001s (RevLUT)\n",
      "Running AOT estimation for band 3 (rhot_560)\n",
      "S2A_MSI/B3 ACOLITE-LUT-202110-MOD1 took 0.001s (RevLUT)\n",
      "S2A_MSI/B3 ACOLITE-LUT-202110-MOD2 took 0.002s (RevLUT)\n",
      "Running AOT estimation for band 4 (rhot_665)\n",
      "S2A_MSI/B4 ACOLITE-LUT-202110-MOD1 took 0.001s (RevLUT)\n",
      "S2A_MSI/B4 ACOLITE-LUT-202110-MOD2 took 0.001s (RevLUT)\n",
      "Running AOT estimation for band 5 (rhot_704)\n",
      "S2A_MSI/B5 ACOLITE-LUT-202110-MOD1 took 0.003s (RevLUT)\n",
      "S2A_MSI/B5 ACOLITE-LUT-202110-MOD2 took 0.001s (RevLUT)\n",
      "Running AOT estimation for band 6 (rhot_740)\n",
      "S2A_MSI/B6 ACOLITE-LUT-202110-MOD1 took 0.001s (RevLUT)\n",
      "S2A_MSI/B6 ACOLITE-LUT-202110-MOD2 took 0.001s (RevLUT)\n",
      "Running AOT estimation for band 7 (rhot_783)\n",
      "S2A_MSI/B7 ACOLITE-LUT-202110-MOD1 took 0.001s (RevLUT)\n",
      "S2A_MSI/B7 ACOLITE-LUT-202110-MOD2 took 0.001s (RevLUT)\n",
      "Running AOT estimation for band 8 (rhot_833)\n",
      "S2A_MSI/B8 ACOLITE-LUT-202110-MOD1 took 0.001s (RevLUT)\n",
      "S2A_MSI/B8 ACOLITE-LUT-202110-MOD2 took 0.002s (RevLUT)\n",
      "Running AOT estimation for band 8A (rhot_865)\n",
      "S2A_MSI/B8A ACOLITE-LUT-202110-MOD1 took 0.001s (RevLUT)\n",
      "S2A_MSI/B8A ACOLITE-LUT-202110-MOD2 took 0.001s (RevLUT)\n",
      "Choosing best fitting model: min_drmsd (2 bands)\n",
      "Computing RMSD for model ACOLITE-LUT-202110-MOD1\n",
      "Computing RMSD for model ACOLITE-LUT-202110-MOD2\n",
      "Selecting most common model for processing.\n",
      "ACOLITE-LUT-202110-MOD1: 42.3%: mean aot of subset = 0.68\n",
      "ACOLITE-LUT-202110-MOD2: 57.7%: mean aot of subset = 0.52\n",
      "Selected ACOLITE-LUT-202110-MOD2, mean aot = 0.56\n",
      "use_revlut True\n",
      "Wrote rhot_443 (3000, 3049)\n",
      "Computing surface reflectance 1 443 0.998\n",
      "Interpolating tiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/working/./acolite/acolite/acolite/acolite_l2r.py:1001: RuntimeWarning: Mean of empty slice\n",
      "  cur_sel_par = np.sqrt(np.nanmean(np.square((rhod_f-rhop_f)), axis=2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote rhos_443 (3000, 3049)\n",
      "S2A_MSI/B1 took 3.8s (RevLUT)\n",
      "Wrote rhot_492 (3000, 3049)\n",
      "Computing surface reflectance 2 492 0.980\n",
      "Interpolating tiles\n",
      "Wrote rhos_492 (3000, 3049)\n",
      "S2A_MSI/B2 took 3.8s (RevLUT)\n",
      "Wrote rhot_560 (3000, 3049)\n",
      "Computing surface reflectance 3 560 0.921\n",
      "Interpolating tiles\n",
      "Wrote rhos_560 (3000, 3049)\n",
      "S2A_MSI/B3 took 3.7s (RevLUT)\n",
      "Wrote rhot_665 (3000, 3049)\n",
      "Computing surface reflectance 4 665 0.953\n",
      "Interpolating tiles\n",
      "Wrote rhos_665 (3000, 3049)\n",
      "S2A_MSI/B4 took 3.8s (RevLUT)\n",
      "Wrote rhot_704 (3000, 3049)\n",
      "Computing surface reflectance 5 704 0.946\n",
      "Interpolating tiles\n",
      "Wrote rhos_704 (3000, 3049)\n",
      "S2A_MSI/B5 took 3.7s (RevLUT)\n",
      "Wrote rhot_740 (3000, 3049)\n",
      "Computing surface reflectance 6 740 0.957\n",
      "Interpolating tiles\n",
      "Wrote rhos_740 (3000, 3049)\n",
      "S2A_MSI/B6 took 4.0s (RevLUT)\n",
      "Wrote rhot_783 (3000, 3049)\n",
      "Computing surface reflectance 7 783 0.983\n",
      "Interpolating tiles\n",
      "Wrote rhos_783 (3000, 3049)\n",
      "S2A_MSI/B7 took 4.0s (RevLUT)\n",
      "Wrote rhot_833 (3000, 3049)\n",
      "Computing surface reflectance 8 833 0.936\n",
      "Interpolating tiles\n",
      "Wrote rhos_833 (3000, 3049)\n",
      "S2A_MSI/B8 took 4.0s (RevLUT)\n",
      "Wrote rhot_865 (3000, 3049)\n",
      "Computing surface reflectance 8A 865 0.997\n",
      "Interpolating tiles\n",
      "Wrote rhos_865 (3000, 3049)\n",
      "S2A_MSI/B8A took 3.8s (RevLUT)\n",
      "Wrote rhot_945 (3000, 3049)\n",
      "Band 9 at 945 nm has tgas < min_tgas_rho (0.27 < 0.70)\n",
      "Wrote rhot_1373 (3000, 3049)\n",
      "Band 10 at 1373 nm has tgas < min_tgas_rho (0.01 < 0.70)\n",
      "Wrote rhot_1614 (3000, 3049)\n",
      "Computing surface reflectance 11 1614 0.958\n",
      "Interpolating tiles\n",
      "Wrote rhos_1614 (3000, 3049)\n",
      "S2A_MSI/B11 took 3.9s (RevLUT)\n",
      "Wrote rhot_2202 (3000, 3049)\n",
      "Computing surface reflectance 12 2202 0.910\n",
      "Interpolating tiles\n",
      "Wrote rhos_2202 (3000, 3049)\n",
      "S2A_MSI/B12 took 3.9s (RevLUT)\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R.nc\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_sza.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_vza.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_raa.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhot_443.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhos_443.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhot_492.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhos_492.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhot_560.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhos_560.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhot_665.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhos_665.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhot_704.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhos_704.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhot_740.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhos_740.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhot_783.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhos_783.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhot_833.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhos_833.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhot_865.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhos_865.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhot_945.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhot_1373.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhot_1614.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhos_1614.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhot_2202.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rhos_2202.tif\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rgb_rhot.png\n",
      "Wrote /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R_rgb_rhos.png\n",
      "Deleting /kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L1R.nc\n",
      "Generated output files {0: {'input': '/kaggle/input/litter-windrows-batch-cala/S2A_MSIL1C_20181017T094011_N0500_R036_T33SXC_20230729T163213/S2A_MSIL1C_20181017T094011_N0500_R036_T33SXC_20230729T163213.SAFE', 'l1r': ['/kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L1R.nc'], 'l2r': ['/kaggle/working/acolite_output//S2A_MSI_2018_10_17_09_49_59_T33SXC_L2R.nc']}}\n",
      "stacking bands\n",
      "Stacked GeoTIFF saved to: /kaggle/working/stacked/S2_20181017T094011_N0500_stacked.tiff\n",
      "Stacked GeoTIFF:\n",
      "Bounds: BoundingBox(left=600000.0, bottom=4216010.0, right=630490.0, top=4246010.0)\n",
      "Dimensions: 3049 columns, 3000 rows\n",
      "Resolution: (10.0, 10.0)\n",
      "CRS: EPSG:32633\n",
      "Number of bands: 11\n",
      "Band descriptions: ('B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12')\n",
      "Percentage of invalid values (NaN, Inf, negative) across all bands: 8.29%\n",
      "patch dir /kaggle/working/patches/S2_20181017T094011_N0500\n",
      "cropping  patch (1024, 7424, 1280, 7680, 42)\n",
      "patch pathname /kaggle/working/patches/S2_20181017T094011_N0500/S2_20181017T094011_N0500_610240_4223220.tif\n",
      "Cropped GeoTIFF saved to: /kaggle/working/patches/S2_20181017T094011_N0500/S2_20181017T094011_N0500_610240_4223220.tif\n",
      "Cropped GeoTIFF:\n",
      "Bounds: BoundingBox(left=610240.0, bottom=4223220.0, right=612800.0, top=4225780.0)\n",
      "Dimensions: 256 columns, 256 rows\n",
      "Resolution: (10.0, 10.0)\n",
      "CRS: EPSG:32633\n",
      "Number of bands: 11\n",
      "Band descriptions: ('B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12')\n",
      "patch mask shape (256, 256)\n",
      "\n",
      "Disk usage after run:\n",
      "5.7G\t/kaggle/input/litter-windrows-batch-cala\n",
      "68M\t/kaggle/input/lw-parquet\n",
      "df: /kaggle/tmp: No such file or directory\n",
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "/dev/loop1       20G  3.5G   17G  18% /kaggle/working\n"
     ]
    }
   ],
   "source": [
    "process_batch([tiles[5]],safe_files_dir, annotations_files,  batch_id='', dagshub_token='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T16:04:52.762482Z",
     "iopub.status.busy": "2025-04-23T16:04:52.761393Z",
     "iopub.status.idle": "2025-04-23T16:05:58.635035Z",
     "shell.execute_reply": "2025-04-23T16:05:58.634321Z",
     "shell.execute_reply.started": "2025-04-23T16:04:52.762390Z"
    }
   },
   "outputs": [],
   "source": [
    "# will this be fully automated in process_tile with dagshubtoken = dagshubtoken?\n",
    "upload_to_dagshub('/kaggle/working', '/kaggle/working/patches', './LWC_DataSet/test_data', \"\", dagshub_token=dagshub_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sagar's & Navodita's added matching visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fdi_from_tiff(tiff_path):\n",
    "    with rasterio.open(tiff_path) as src:\n",
    "        # Assuming band order follows your stacked TIFF (B1–B12, skipping B10 if needed)\n",
    "        # Band indices are 1-based in rasterio\n",
    "        R665 = src.read(4)    # B4\n",
    "        R859 = src.read(9)    # B8A\n",
    "        R1610 = src.read(10)  # B11\n",
    "        # Convert to float and mask invalid values\n",
    "        R665 = R665.astype(np.float32)\n",
    "        R859 = R859.astype(np.float32)\n",
    "        R1610 = R1610.astype(np.float32)\n",
    "        # Calculate FDI\n",
    "        FDI = R859 - (R665 + ((R1610 - R665) * (859 - 665) / (1610 - 665)))\n",
    "        return FDI\n",
    "def compute_ndwi(tiff_path):\n",
    "    with rasterio.open(tiff_path) as src:\n",
    "        Rgreen = src.read(3).astype(np.float32)  # Band 3 (Green)\n",
    "        Rnir = src.read(8).astype(np.float32)    # Band 8 (NIR)\n",
    "        ndwi = (Rgreen - Rnir) / (Rgreen + Rnir + 1e-6)  # avoid divide-by-zero\n",
    "    return ndwi\n",
    "def plot_fdi(fdi_array, ndwi, img_path, mask_path):\n",
    "    with rasterio.open(img_path) as src:\n",
    "        rgb = src.read([4, 3, 2])\n",
    "        rgb = np.transpose(rgb, (1, 2, 0))\n",
    "    # Normalization\n",
    "    rgb = rgb.astype(np.float32)\n",
    "    rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n",
    "    with rasterio.open(mask_path) as src:\n",
    "        mask = src.read(1)\n",
    "    # Create binary mask\n",
    "    mask_binary = mask > 0\n",
    "    # Plot side-by-side\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "    axs[0].imshow(rgb)\n",
    "    axs[0].set_title(\"RGB Patch\")\n",
    "    axs[1].imshow(mask_binary)  #, cmap='gray')\n",
    "    axs[1].set_title(\"Binary Mask (._cl.tif)\")\n",
    "    axs[2].imshow(fdi_array)\n",
    "    axs[2].set_title(\"FDI\")\n",
    "    axs[3].imshow(ndwi)\n",
    "    axs[3].set_title(\"NDWI\")\n",
    "    for ax in axs:\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualises RGB, mask, FDI and NDWI\n",
    "# Edit paths to loop over all patches in folder\n",
    "\n",
    "image_path = '/kaggle/working/patches/S2_20190531T100031_N0500/S2_20190531T100031_N0500_305120_5002720.tif'\n",
    "mask_path = '/kaggle/working/patches/S2_20190531T100031_N0500/S2_20190531T100031_N0500_305120_5002720_cl.tif'\n",
    "fdi = compute_fdi_from_tiff(image_path)\n",
    "ndwi = compute_ndwi(image_path)\n",
    "plot_fdi(fdi, ndwi, image_path, mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualises RGB, mask, and matched overlay\n",
    "# Edit paths to loop over all patches in folder and combine with FDI \n",
    "patch_dir = \"/kaggle/working/patches/S2_20190531T100031_N0500\"\n",
    "patch_paths = sorted([p for p in glob(os.path.join(patch_dir, \"*.tif\")) if not p.endswith('*_cl.tif')])\n",
    "mask_paths = sorted(glob(os.path.join(patch_dir, \"*_cl.tif\")))\n",
    "# Ensure both have same count\n",
    "print(f\"Found {len(patch_paths)} RGB patches and {len(mask_paths)} masks.\")\n",
    "# Plot Patch, Mask, and Overlay side-by-side\n",
    "debris_pixel_counts = []\n",
    "plt.figure(figsize=(15, 5 * len(mask_paths)))\n",
    "for i, (patch_path, mask_path) in enumerate(zip(patch_paths, mask_paths)):\n",
    "    with rasterio.open(patch_path) as patch_src:\n",
    "        rgb = patch_src.read([4, 3, 2])  # Use bands B4, B3, B2 for RGB\n",
    "        rgb = np.transpose(rgb, (1, 2, 0))\n",
    "        rgb = (rgb - np.min(rgb)) / (np.max(rgb) - np.min(rgb) + 1e-6)\n",
    "    with rasterio.open(mask_path) as mask_src:\n",
    "        mask = mask_src.read(1)\n",
    "        mask_binary = (mask > 0).astype(np.uint8)\n",
    "    debris_pixel_counts.append(np.sum(mask_binary))\n",
    "    # Create overlay manually\n",
    "    overlay = rgb.copy()\n",
    "    overlay[mask_binary == 1] = [1.0, 0.0, 0.0]  # Red color on debris\n",
    "    # Plot\n",
    "    plt.subplot(len(mask_paths), 3, 3*i+1)\n",
    "    plt.imshow(rgb)\n",
    "    plt.title(f\"Patch\\n{os.path.basename(patch_path)}\")\n",
    "    plt.axis('off')\n",
    "    plt.subplot(len(mask_paths), 3, 3*i+2)\n",
    "    plt.imshow(mask_binary, cmap='gray')\n",
    "    plt.title(f\"Mask\\nDebris pixels: {int(np.sum(mask_binary))}\")\n",
    "    plt.axis('off')\n",
    "    plt.subplot(len(mask_paths), 3, 3*i+3)\n",
    "    plt.imshow(overlay)\n",
    "    plt.title(\"Overlay: Mask on Patch\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Histogram for debris pixels\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(debris_pixel_counts, bins=10, color='teal', edgecolor='black')\n",
    "plt.title(\"Histogram of Debris Pixels per Patch\")\n",
    "plt.xlabel(\"Debris Pixels\")\n",
    "plt.ylabel(\"Number of Patches\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7163561,
     "sourceId": 11436527,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7191993,
     "sourceId": 11475439,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
